{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset Pricing — Week 3\n",
    "## Long-Run Risks, Disaster Risk, and the Factor Zoo\n",
    "\n",
    "---\n",
    "\n",
    "Week 2 ended with a precise diagnosis: the CCAPM fails because aggregate consumption is too smooth to generate the observed equity premium at plausible risk aversion. Two structural remedies were introduced — habit formation and Epstein-Zin preferences. But habit formation requires extreme effective risk aversion in downturns, and Epstein-Zin under i.i.d. consumption growth does not resolve the equity premium puzzle.\n",
    "\n",
    "This week develops two of the most quantitatively successful equilibrium models in the literature, and then transitions from equilibrium theory to the empirical cross-section of expected returns.\n",
    "\n",
    "**Part A — Long-Run Risks (Bansal-Yaron, 2004)**: Combines Epstein-Zin preferences with a consumption process containing small, persistent components and stochastic volatility. The IES greater than one makes agents sensitive to long-horizon news, amplifying risk premia without extreme risk aversion.\n",
    "\n",
    "**Part B — Disaster Risk (Rietz, 1988; Barro, 2006)**: Introduces rare, large consumption disasters that are nearly invisible in postwar US data but whose risk is priced. A small probability of catastrophic loss can generate large equity premia with modest risk aversion.\n",
    "\n",
    "**Part C — The Arbitrage Pricing Theory and Factor Models**: Moves from equilibrium theory to empirical factor models. The APT provides a no-arbitrage foundation; the Fama-French models provide empirical implementations. We develop the tools for evaluating whether a factor model spans the mean-variance frontier.\n",
    "\n",
    "**Prerequisites**: Weeks 1 and 2. The material in Part A requires comfort with lognormal moment generating functions and the Epstein-Zin SDF derived in Week 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.optimize import minimize, brentq\n",
    "from scipy.stats import norm, t as t_dist\n",
    "from scipy.linalg import solve, cholesky\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.size': 11,\n",
    "})\n",
    "\n",
    "rng = np.random.default_rng(seed=314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A — Long-Run Risks\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Core Insight: Why Long-Run Risk Matters\n",
    "\n",
    "Recall from Week 2 that the Epstein-Zin SDF is:\n",
    "$$\n",
    "M_{t+1} = \\beta^{\\theta} \\left(\\frac{C_{t+1}}{C_t}\\right)^{-\\theta/\\psi} R_{w,t+1}^{\\theta-1},\n",
    "\\quad \\theta \\equiv \\frac{1-\\gamma}{1-1/\\psi}.\n",
    "$$\n",
    "When $\\psi > 1$ (high IES) and $\\gamma > 1$, we have $\\theta < 0$. The SDF is *decreasing* in the wealth return $R_{w,t+1}$. This is the critical mechanism: when the wealth portfolio falls (bad news), $M$ rises, generating risk premia on assets that covary negatively with wealth.\n",
    "\n",
    "Under i.i.d. consumption growth, the wealth return $R_w$ is simply a scaled version of consumption growth. The SDF then reduces to something close to the CRRA SDF — and we showed in Week 2 that this does not resolve the equity premium puzzle.\n",
    "\n",
    "**The long-run risks idea (Bansal and Yaron, 2004)**: Suppose consumption growth contains a small but *highly persistent* component. Even if this component is tiny (undetectable in short samples), it generates large variation in the wealth portfolio return — because the wealth portfolio prices all future consumption. An agent with EZ preferences and $\\psi > 1$ cares about this long-horizon risk, and demands a large premium for bearing it.\n",
    "\n",
    "The analogy is useful: a small leak in a boat that will accumulate over 30 years is trivially undetectable day-to-day, but the long-run consequences are enormous. An agent who thinks far ahead prices this risk heavily.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Bansal-Yaron (2004) Model\n",
    "\n",
    "### 2.1 The Consumption and Dividend Process\n",
    "\n",
    "Log consumption growth $g_{t+1} = \\ln(C_{t+1}/C_t)$ and log dividend growth $g^d_{t+1} = \\ln(D_{t+1}/D_t)$ follow:\n",
    "\n",
    "$$\n",
    "g_{t+1} = \\mu_c + x_t + \\sigma_t \\eta_{t+1}\n",
    "$$\n",
    "$$\n",
    "g^d_{t+1} = \\mu_d + \\phi x_t + \\varphi \\sigma_t u_{t+1}\n",
    "$$\n",
    "$$\n",
    "x_{t+1} = \\rho x_t + \\phi_e \\sigma_t e_{t+1}\n",
    "$$\n",
    "$$\n",
    "\\sigma^2_{t+1} = \\bar{\\sigma}^2 + \\nu_1 (\\sigma^2_t - \\bar{\\sigma}^2) + \\sigma_w w_{t+1}\n",
    "$$\n",
    "\n",
    "where $\\eta_{t+1}$, $u_{t+1}$, $e_{t+1}$, $w_{t+1}$ are i.i.d. $\\mathcal{N}(0,1)$ shocks, independent of each other.\n",
    "\n",
    "**State variables**:\n",
    "- $x_t$: the **long-run component** of consumption growth. It follows an AR(1) with persistence $\\rho \\approx 0.979$ (annual). Its variance is tiny relative to the short-run shock $\\sigma_t \\eta_{t+1}$, but because $\\rho$ is close to one, it has a large cumulative effect on the level of consumption.\n",
    "- $\\sigma^2_t$: **stochastic volatility** of consumption growth. When $\\sigma_t$ is high, both consumption shocks and long-run risk shocks are amplified.\n",
    "\n",
    "**Key parameters**:\n",
    "- $\\phi > 1$: dividends are levered consumption. A 1% shock to $x_t$ produces a $\\phi\\%$ shock to dividend growth. This leverage amplifies risk premia on the dividend claim (equity) relative to the consumption claim.\n",
    "- $\\varphi > 1$: dividends are also more volatile than consumption (additional idiosyncratic volatility).\n",
    "\n",
    "### 2.2 Why Long-Run Risk Is Priced\n",
    "\n",
    "The wealth portfolio $R_w$ prices the claim to all future consumption. When $x_t$ rises, the market revises *all future* expected consumption growth upward. The value of the wealth portfolio rises substantially — much more than would be warranted by the current-period shock alone.\n",
    "\n",
    "Under $\\psi > 1$: the agent prefers early resolution of uncertainty (EZ with $\\gamma > 1/\\psi$ values knowing bad news early). News about the persistent component $x_t$ therefore directly enters the SDF through $R_w$. The equity premium has two components:\n",
    "\n",
    "$$\n",
    "E[R^e] - R_f = \\underbrace{\\text{(short-run risk premium)}}_{\\propto \\text{Cov}(g_t, r^e_t)} + \\underbrace{\\text{(long-run risk premium)}}_{\\propto \\text{Cov}(x_t, r^e_t) \\cdot \\text{(persistence loading)}}.\n",
    "$$\n",
    "\n",
    "The long-run risk premium can be large even when the short-run covariance between consumption growth and equity returns is small — resolving the Mehra-Prescott finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bansal-Yaron (2004) calibration (monthly, Table 1)\n",
    "# Parameters\n",
    "mu_c    = 0.0015    # mean monthly consumption growth\n",
    "mu_d    = 0.0015    # mean monthly dividend growth\n",
    "rho     = 0.979**(1/12)  # monthly persistence of x (annualised 0.979)\n",
    "phi_e   = 0.044     # volatility of shock to x, scaled by sigma\n",
    "phi_d   = 3.0       # leverage (dividend loading on x)\n",
    "varphi  = 4.5       # extra dividend vol\n",
    "nu1     = 0.987**(1/12)  # monthly persistence of sigma^2\n",
    "sigma_bar = np.sqrt(0.0072**2)  # unconditional monthly std of consumption growth\n",
    "sigma_w   = 0.23e-5  # vol of vol shock\n",
    "\n",
    "# Preferences\n",
    "gamma_by = 10.0\n",
    "psi_by   = 1.5\n",
    "beta_by  = 0.998   # monthly\n",
    "theta_by = (1 - gamma_by) / (1 - 1/psi_by)\n",
    "\n",
    "T_by = 12 * 300   # 300 years monthly\n",
    "\n",
    "# Simulate the state variables\n",
    "eta  = rng.standard_normal(T_by)\n",
    "e    = rng.standard_normal(T_by)\n",
    "u    = rng.standard_normal(T_by)\n",
    "w    = rng.standard_normal(T_by)\n",
    "\n",
    "x      = np.empty(T_by + 1)\n",
    "sig2   = np.empty(T_by + 1)\n",
    "x[0]   = 0.0\n",
    "sig2[0] = sigma_bar**2\n",
    "\n",
    "g_c = np.empty(T_by)   # consumption growth\n",
    "g_d = np.empty(T_by)   # dividend growth\n",
    "\n",
    "for t in range(T_by):\n",
    "    sig = np.sqrt(max(sig2[t], 1e-8))\n",
    "    g_c[t] = mu_c + x[t] + sig * eta[t]\n",
    "    g_d[t] = mu_d + phi_d * x[t] + varphi * sig * u[t]\n",
    "    x[t+1]    = rho * x[t] + phi_e * sig * e[t]\n",
    "    sig2[t+1] = sigma_bar**2 + nu1 * (sig2[t] - sigma_bar**2) + sigma_w * w[t]\n",
    "    sig2[t+1] = max(sig2[t+1], 1e-8)\n",
    "\n",
    "sig_arr = np.sqrt(sig2[:-1])\n",
    "\n",
    "# Annualise moments for inspection\n",
    "g_c_ann = np.array([g_c[i*12:(i+1)*12].sum() for i in range(T_by//12)])\n",
    "g_d_ann = np.array([g_d[i*12:(i+1)*12].sum() for i in range(T_by//12)])\n",
    "x_ann   = x[::12]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Simulated Bansal-Yaron Process — Annual Moments\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  E[g_c]          = {g_c_ann.mean()*100:.3f}%   (data: ~1.8%)\")\n",
    "print(f\"  std(g_c)        = {g_c_ann.std()*100:.3f}%   (data: ~2.7%)\")\n",
    "print(f\"  AC(1) of g_c    = {np.corrcoef(g_c_ann[:-1], g_c_ann[1:])[0,1]:.3f}   (near 0 in data)\")\n",
    "print(f\"  E[g_d]          = {g_d_ann.mean()*100:.3f}%\")\n",
    "print(f\"  std(g_d)        = {g_d_ann.std()*100:.3f}%   (data: ~11%)\")\n",
    "print(f\"  Corr(g_c, g_d)  = {np.corrcoef(g_c_ann, g_d_ann)[0,1]:.3f}\")\n",
    "print(f\"  std(x_t) annual = {x_ann.std()*100:.3f}%   (tiny relative to g_c shocks)\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "years = np.arange(T_by // 12)\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(years[:100], g_c_ann[:100] * 100, lw=1.2, color='steelblue')\n",
    "ax.set_xlabel('Year'); ax.set_ylabel('Log consumption growth (%)')\n",
    "ax.set_title('Consumption Growth\\n(first 100 years)')\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(years[:100], x_ann[:100] * 1000, lw=1.5, color='#e74c3c')\n",
    "ax2.axhline(0, color='black', lw=0.8, ls='--')\n",
    "ax2.set_xlabel('Year'); ax2.set_ylabel('Long-run component $x_t$ (×1000)')\n",
    "ax2.set_title('Long-Run Component $x_t$\\n(persistent, tiny in magnitude)')\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(years[:100], g_d_ann[:100] * 100, lw=1.0, color='#27ae60', alpha=0.8)\n",
    "ax3.set_xlabel('Year'); ax3.set_ylabel('Log dividend growth (%)')\n",
    "ax3.set_title('Dividend Growth\\n(levered, more volatile than consumption)')\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "sig2_ann = np.array([sig2[i*12] for i in range(T_by//12)])\n",
    "ax4.plot(years[:100], np.sqrt(sig2_ann[:100]) * 100 * np.sqrt(12), lw=1.5, color='#8e44ad')\n",
    "ax4.axhline(sigma_bar * 100 * np.sqrt(12), color='black', lw=1, ls='--',\n",
    "            label=f'Unconditional σ = {sigma_bar*100*np.sqrt(12):.2f}% p.a.')\n",
    "ax4.set_xlabel('Year'); ax4.set_ylabel('Annualised consumption vol (%)')\n",
    "ax4.set_title('Stochastic Volatility $\\\\sigma_t$')\n",
    "ax4.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Bansal-Yaron (2004) State Variables', y=1.01, fontsize=13, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Log-Linearisation and Analytical Results\n",
    "\n",
    "The Bansal-Yaron model does not have a closed-form solution for asset prices. The standard approach is **Campbell-Shiller log-linearisation** of the return identity:\n",
    "$$\n",
    "r_{w,t+1} \\equiv \\ln R_{w,t+1} \\approx \\kappa_0 + \\kappa_1 z_{t+1} - z_t + g_{t+1},\n",
    "$$\n",
    "where $z_t = \\ln(P_t/C_t)$ is the log consumption-wealth ratio (the variable to be solved for) and $\\kappa_0$, $\\kappa_1 \\in (0,1)$ are linearisation constants.\n",
    "\n",
    "**Guess**: The log price-consumption ratio is linear in the state variables:\n",
    "$$\n",
    "z_t = A_0 + A_1 x_t + A_2 \\sigma^2_t.\n",
    "$$\n",
    "Substituting into the Euler equation for the wealth portfolio ($E_t[M_{t+1} R_{w,t+1}] = 1$, which in logs becomes $E_t[m_{t+1} + r_{w,t+1}] + \\frac{1}{2}\\text{Var}_t[m_{t+1} + r_{w,t+1}] = 0$), one can match coefficients to obtain:\n",
    "\n",
    "$$\n",
    "A_1 = \\frac{1 - 1/\\psi}{1 - \\kappa_1 \\rho}, \\qquad\n",
    "A_2 = \\frac{(1-\\gamma)(1 - 1/(2\\theta))}{1 - \\kappa_1 \\nu_1} \\cdot \\frac{\\phi_e^2/2 \\cdot A_1^2 + \\ldots}{\\ldots}.\n",
    "$$\n",
    "\n",
    "**Key interpretation of $A_1$**:\n",
    "- When $\\psi > 1$: $1 - 1/\\psi > 0$, so $A_1 > 0$. A positive shock to $x_t$ (higher expected future growth) *raises* the price-consumption ratio. The agent, anticipating better times, values the wealth claim more.\n",
    "- When $\\psi < 1$: $A_1 < 0$. The agent with low IES actually *lowers* the price-consumption ratio when growth improves — because they want to smooth consumption, they save less when times are expected to be good. This is the key reason the model requires $\\psi > 1$: only then does good news about growth raise equity prices, making equity a bad hedge in bad times (negative $A_1$, bad times have low $x$, low $P/C$, low equity prices).\n",
    "\n",
    "### 2.4 Sources of the Risk Premium\n",
    "\n",
    "The log return on the equity claim (dividend claim) can similarly be log-linearised as:\n",
    "$$\n",
    "r^e_{t+1} \\approx \\kappa^e_0 + \\kappa^e_1 z^e_{t+1} - z^e_t + g^d_{t+1},\n",
    "$$\n",
    "where $z^e_t = A^e_0 + A^e_1 x_t + A^e_2 \\sigma^2_t$ is the log price-dividend ratio.\n",
    "\n",
    "The equity risk premium decomposes into three terms:\n",
    "$$\n",
    "E_t[r^e_{t+1}] - r_{f,t} + \\frac{1}{2}\\text{Var}_t[r^e_{t+1}] =\n",
    "\\underbrace{\\gamma \\, \\beta_{\\eta}^e \\, \\sigma^2_t}_{\\text{Short-run risk}}\n",
    "+ \\underbrace{(1-\\gamma)\\kappa_1 A_1 \\phi_e \\, \\beta_{e,\\sigma}^e \\, \\sigma^2_t}_{\\text{Long-run risk}}\n",
    "+ \\underbrace{(\\ldots) \\sigma_w}_{\\text{Vol-of-vol risk}},\n",
    "$$\n",
    "where $\\beta^e_{\\eta}$ and $\\beta^e_{e}$ are the loadings of equity returns on the short-run ($\\eta$) and long-run ($e$) consumption shocks.\n",
    "\n",
    "Because the long-run shock has leverage $\\phi_d$ and is amplified by the factor $(1-\\kappa_1\\rho)^{-1}$ (the persistence amplifier), even a small $\\phi_e$ generates a large long-run risk premium when $\\rho$ is close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the Bansal-Yaron model via log-linearisation\n",
    "# Approximate A1, A2 for the wealth portfolio and the equity claim\n",
    "\n",
    "# Log-linearisation constant kappa_1 depends on the mean log PD ratio\n",
    "# We iterate to find a fixed point\n",
    "\n",
    "def solve_by_model(gamma, psi, beta, rho, phi_e, phi_d, varphi,\n",
    "                   mu_c, sigma_bar, nu1, sigma_w):\n",
    "    \"\"\"\n",
    "    Solve Bansal-Yaron model via log-linearisation.\n",
    "    Returns: (rf, ep, sigma_ep, kappa) dictionary of annualised moments.\n",
    "    \"\"\"\n",
    "    theta = (1 - gamma) / (1 - 1/psi)\n",
    "\n",
    "    # Iterate on kappa_1 (Campbell-Shiller linearisation constant)\n",
    "    kappa1 = 0.997  # starting guess (monthly)\n",
    "    for _ in range(200):\n",
    "        # Solve for A1 (loading of log P/C on x_t)\n",
    "        A1 = (1 - 1/psi) / (1 - kappa1 * rho)\n",
    "\n",
    "        # Solve for A2 (loading of log P/C on sigma^2_t)\n",
    "        # Numerator: variance terms from the Euler equation\n",
    "        # sigma^2 coefficient in the SDF:\n",
    "        # Var_t[m_{t+1}] / 2 + covariance terms\n",
    "        # Simplified: A2 = theta * [0.5*(A1^2 * phi_e^2 + theta^{-1}*(1-gamma)^{-1}*...)] / (1 - kappa1*nu1)\n",
    "        # Using BY (2004) Appendix expressions:\n",
    "        numer_A2 = 0.5 * theta * (kappa1 * A1 * phi_e)**2 + 0.5 * (1 - gamma/psi)**2 - 0.5\n",
    "        A2 = numer_A2 / (1 - kappa1 * nu1)\n",
    "\n",
    "        # Update kappa_1 from the mean log PD ratio\n",
    "        # Mean z = A0 + A2 * sigma_bar^2 (A1*E[x]=0)\n",
    "        # kappa_0 + kappa_1*z_ss = r_w - g_c  (in steady state)\n",
    "        # By definition: exp(z_ss) / (exp(z_ss)+1) = kappa_1 (Campbell-Shiller)\n",
    "        # A0 is determined by the riskfree rate equation (not needed for kappa)\n",
    "        # Approximate: use log(P/C) ≈ log(P/D) for calibration\n",
    "        mean_z = np.log(20.0)  # approximate; use data PD ~20 monthly\n",
    "        kappa1_new = np.exp(mean_z) / (1 + np.exp(mean_z))\n",
    "        if abs(kappa1_new - kappa1) < 1e-8:\n",
    "            break\n",
    "        kappa1 = 0.5 * kappa1 + 0.5 * kappa1_new\n",
    "\n",
    "    # Risk-free rate (monthly, conditional on sigma^2_t = sigma_bar^2)\n",
    "    # r_f = -log(beta) + mu_c/psi - (1/2) * (gamma/psi)^2 * sigma^2\n",
    "    #       - (1/2) * theta * (kappa1 * A1 * phi_e)^2 * sigma^2\n",
    "    #       - A2 * (1 - nu1) * sigma^2\n",
    "    sigma2_bar = sigma_bar**2\n",
    "    rf_monthly = (-np.log(beta)\n",
    "                  + mu_c / psi\n",
    "                  - 0.5 * (gamma/psi)**2 * sigma2_bar\n",
    "                  - 0.5 * theta * (kappa1 * A1 * phi_e)**2 * sigma2_bar\n",
    "                  + A2 * sigma_w)  # vol-of-vol correction (small)\n",
    "\n",
    "    # Equity: solve A1^e, A2^e for dividend claim\n",
    "    # phi_d leverage on x; kappa1^e ≈ kappa1 (same linearisation)\n",
    "    kappa1_e = kappa1\n",
    "    A1_e = (phi_d - 1/psi) / (1 - kappa1_e * rho)\n",
    "    numer_A2_e = (0.5 * theta * (kappa1 * A1 * phi_e)**2\n",
    "                  + 0.5 * (varphi**2 - (gamma/psi)**2 + (kappa1_e * A1_e * phi_e)**2 * theta))\n",
    "    A2_e = numer_A2_e / (1 - kappa1_e * nu1)\n",
    "\n",
    "    # Equity premium (monthly)\n",
    "    # EP = gamma*beta_eta*sigma^2 + (gamma - theta*kappa1*A1*phi_e)*(kappa1_e*A1_e*phi_e)*sigma^2\n",
    "    # Simplified (BY Appendix C):\n",
    "    beta_eta_e   = 1.0          # equity loading on short-run shock (normalised)\n",
    "    ep_sr = gamma * sigma2_bar  # short-run contribution\n",
    "    ep_lr = (gamma - theta * kappa1 * A1 * phi_e) * (kappa1_e * A1_e * phi_e) * sigma2_bar\n",
    "    ep_monthly = ep_sr + ep_lr\n",
    "\n",
    "    return {\n",
    "        'rf':    rf_monthly * 12 * 100,\n",
    "        'ep':    ep_monthly * 12 * 100,\n",
    "        'A1':    A1,\n",
    "        'A2':    A2,\n",
    "        'kappa1': kappa1,\n",
    "        'ep_sr_share': ep_sr / (ep_sr + ep_lr + 1e-12),\n",
    "        'ep_lr_share': ep_lr / (ep_sr + ep_lr + 1e-12),\n",
    "    }\n",
    "\n",
    "# Baseline calibration\n",
    "res_base = solve_by_model(gamma_by, psi_by, beta_by, rho, phi_e, phi_d, varphi,\n",
    "                          mu_c, sigma_bar, nu1, sigma_w)\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"Bansal-Yaron Model — Analytical Moments (annualised)\")\n",
    "print(f\"  Parameters: γ={gamma_by}, ψ={psi_by}, β_monthly={beta_by}\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"  Riskfree rate rf:           {res_base['rf']:.2f}%   (data: ~0.9%)\")\n",
    "print(f\"  Equity premium EP:          {res_base['ep']:.2f}%   (data: ~6.0%)\")\n",
    "print(f\"  Short-run risk share:       {res_base['ep_sr_share']*100:.1f}%\")\n",
    "print(f\"  Long-run risk share:        {res_base['ep_lr_share']*100:.1f}%\")\n",
    "print(f\"  Log P/C loading on x (A1):  {res_base['A1']:.2f}\")\n",
    "print(f\"  Log-linearisation κ₁:       {res_base['kappa1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis: how do the model moments vary with psi and gamma?\n",
    "psi_range   = np.linspace(0.5, 2.5, 30)\n",
    "gamma_range = np.linspace(5, 25, 30)\n",
    "\n",
    "rf_grid = np.full((len(gamma_range), len(psi_range)), np.nan)\n",
    "ep_grid = np.full((len(gamma_range), len(psi_range)), np.nan)\n",
    "\n",
    "for i, gam in enumerate(gamma_range):\n",
    "    for j, psi in enumerate(psi_range):\n",
    "        try:\n",
    "            res = solve_by_model(gam, psi, beta_by, rho, phi_e, phi_d, varphi,\n",
    "                                 mu_c, sigma_bar, nu1, sigma_w)\n",
    "            if np.isfinite(res['rf']) and np.isfinite(res['ep']):\n",
    "                rf_grid[i, j] = res['rf']\n",
    "                ep_grid[i, j] = res['ep']\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "PSI, GAM = np.meshgrid(psi_range, gamma_range)\n",
    "\n",
    "ax = axes[0]\n",
    "cf1 = ax.contourf(PSI, GAM, np.clip(ep_grid, 0, 15), levels=20, cmap='RdYlGn')\n",
    "ax.contour(PSI, GAM, ep_grid, levels=[6.0], colors='white', linewidths=2)\n",
    "plt.colorbar(cf1, ax=ax, label='Equity premium (% p.a.)')\n",
    "ax.set_xlabel('IES $\\\\psi$')\n",
    "ax.set_ylabel('Risk aversion $\\\\gamma$')\n",
    "ax.set_title('Equity Premium (white = 6% data target)\\nBansal-Yaron Model')\n",
    "ax.scatter([psi_by], [gamma_by], color='white', s=100, marker='*',\n",
    "           label=f'Baseline (ψ={psi_by}, γ={gamma_by})', zorder=5)\n",
    "ax.legend(fontsize=9)\n",
    "ax.axvline(1.0, color='white', lw=1, ls='--', alpha=0.5)\n",
    "ax.text(0.85, 22, 'ψ<1', color='white', fontsize=9)\n",
    "ax.text(1.05, 22, 'ψ>1', color='white', fontsize=9)\n",
    "\n",
    "ax2 = axes[1]\n",
    "cf2 = ax2.contourf(PSI, GAM, np.clip(rf_grid, -5, 10), levels=20, cmap='coolwarm')\n",
    "ax2.contour(PSI, GAM, rf_grid, levels=[0.9], colors='black', linewidths=2)\n",
    "plt.colorbar(cf2, ax=ax2, label='Riskfree rate (% p.a.)')\n",
    "ax2.set_xlabel('IES $\\\\psi$')\n",
    "ax2.set_ylabel('Risk aversion $\\\\gamma$')\n",
    "ax2.set_title('Riskfree Rate (black = 0.9% data target)\\nBansal-Yaron Model')\n",
    "ax2.scatter([psi_by], [gamma_by], color='black', s=100, marker='*',\n",
    "            label=f'Baseline (ψ={psi_by}, γ={gamma_by})', zorder=5)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.axvline(1.0, color='black', lw=1, ls='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The BY model can match BOTH moments jointly: ψ>1 keeps rf low; γ>1 drives the EP.\")\n",
    "print(\"The IES threshold ψ=1 is critical: below it, the equity premium can become negative.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Stochastic Volatility and the Price of Variance Risk\n",
    "\n",
    "The second state variable $\\sigma^2_t$ introduces **time-varying risk premia**. When $\\sigma^2_t$ is high:\n",
    "1. Equity is riskier — both short-run and long-run consumption shocks are amplified.\n",
    "2. The equity risk premium rises.\n",
    "3. The price-dividend ratio falls (via the $A_2 \\sigma^2_t$ term, and $A_2 < 0$ under the calibration).\n",
    "\n",
    "This generates **countercyclical risk premia**: premia are highest in recessions, when volatility is elevated. This is a key empirical feature of financial markets that the model successfully reproduces.\n",
    "\n",
    "Furthermore, $\\sigma^2_t$ is itself a risk factor. The **variance risk premium** — the difference between the risk-neutral and physical expectation of future variance — is:\n",
    "$$\n",
    "\\text{VRP}_t = E^Q_t[\\sigma^2_{t+1}] - E^P_t[\\sigma^2_{t+1}] = -(1-\\theta) \\kappa_1 A_2 \\sigma_w \\cdot \\sigma^2_t.\n",
    "$$\n",
    "Under the calibration, $\\theta < 0$, $A_2 < 0$, so VRP $> 0$ — investors pay a premium to shed variance risk. This prediction aligns with the empirical finding (Bollerslev, Tauchen, Zhou, 2009) that the VRP predicts future excess returns with a positive sign.\n",
    "\n",
    "---\n",
    "\n",
    "# Part B — Disaster Risk\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Rietz-Barro Framework\n",
    "\n",
    "The long-run risks model resolves the equity premium puzzle by introducing a *persistent* small component in consumption growth that is invisible in short samples. A fundamentally different approach is to introduce *rare large disasters* — events that are catastrophic but so infrequent that they barely appear in the historical US record.\n",
    "\n",
    "### 3.1 The Core Argument (Rietz, 1988)\n",
    "\n",
    "Rietz (1988) proposed that the equity premium reflects compensation for the small probability of a catastrophic event (a Great Depression-scale collapse) that would devastate consumption and equity payoffs simultaneously.\n",
    "\n",
    "**Why this works intuitively**: An agent with CRRA utility $\\gamma > 1$ has marginal utility that becomes *very* large when consumption is very low. A 50% drop in consumption raises marginal utility by $2^\\gamma$. At $\\gamma = 4$, this is a 16-fold increase. Even a 1% probability of such an event generates a large risk premium if the payoff in that state is sufficiently bad.\n",
    "\n",
    "Rietz's proposal was criticised by Mehra and Prescott as requiring disaster probabilities that were too high to be realistic. Barro (2006) revived the idea using international historical data.\n",
    "\n",
    "### 3.2 Barro (2006): The International Evidence\n",
    "\n",
    "Barro (2006) compiled data on per-capita GDP contractions of 15% or more from 35 countries over the 20th century. He found:\n",
    "\n",
    "- Probability of a disaster in any given year: $p \\approx 0.017$ (about 1.7%).\n",
    "- Given a disaster, the mean log contraction: $E[\\ln(1-b)] \\approx -0.22$ (about 22%).\n",
    "- Disasters have significant variance: contractions range from 15% to over 60%.\n",
    "\n",
    "Postwar US data covers only about 60 years — so we would expect to observe roughly $0.017 \\times 60 \\approx 1$ disaster on average. The US was lucky to avoid one, but the risk is priced in equity markets.\n",
    "\n",
    "### 3.3 Formal Setup\n",
    "\n",
    "**Consumption process**: Each period, with probability $1 - p$, a normal event occurs and consumption grows as $C_{t+1}/C_t = e^{\\mu_c + \\sigma_c \\varepsilon_{t+1}}$. With probability $p$, a disaster occurs and consumption contracts by a random fraction $b_t \\in (0,1)$:\n",
    "$$\n",
    "C_{t+1}/C_t = \\begin{cases} e^{\\mu_c + \\sigma_c \\varepsilon_{t+1}} & \\text{with prob } 1-p \\\\ (1-b_t) e^{\\mu_c + \\sigma_c \\varepsilon_{t+1}} & \\text{with prob } p. \\end{cases}\n",
    "$$\n",
    "\n",
    "**Equity payoff**: In a disaster, equities can be partially defaulted upon. Let the equity recovery rate be $f \\leq 1$, so the dividend during a disaster falls by a factor $(1-b_t)^\\phi$ for leverage $\\phi \\geq 1$.\n",
    "\n",
    "### 3.4 The Equity Premium Under Disaster Risk\n",
    "\n",
    "With CRRA utility (for clarity), the SDF is:\n",
    "$$\n",
    "M_{t+1} = \\beta \\left(\\frac{C_{t+1}}{C_t}\\right)^{-\\gamma}.\n",
    "$$\n",
    "The riskfree rate and equity premium under i.i.d. Poisson disasters (using the Poisson approximation $p \\ll 1$) are:\n",
    "$$\n",
    "r_f \\approx -\\ln\\beta + \\gamma \\mu_c - \\frac{\\gamma^2 \\sigma_c^2}{2} - p \\cdot E[(1-b)^{-\\gamma} - 1],\n",
    "$$\n",
    "$$\n",
    "E[r^e] - r_f \\approx \\gamma \\sigma_c^2 + p \\cdot E[(1-b)^{-\\gamma} (1 - (1-b)^{\\phi})].\n",
    "$$\n",
    "\n",
    "The first term in the equity premium is the usual CCAPM term — small as before. The second term is the **disaster premium**: it depends on the probability $p$, the risk aversion $\\gamma$, and the leverage $\\phi$. Even with $\\gamma = 3$ (very moderate), a 1.7% disaster probability with a 22% mean contraction can generate a 4–5% equity premium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute equity premium and riskfree rate under disaster risk\n",
    "# Barro (2006) calibration\n",
    "\n",
    "# Normal-times parameters\n",
    "mu_c_barro  = 0.025   # mean log consumption growth (excluding disaster)\n",
    "sigma_c_barro = 0.02  # std of log consumption growth in normal times\n",
    "beta_barro  = 0.95    # annual discount factor\n",
    "p_barro     = 0.017   # annual disaster probability\n",
    "phi_barro   = 3.0     # leverage of dividends\n",
    "\n",
    "# Distribution of disaster size b (log-uniform approximation from Barro 2006)\n",
    "# b is the fractional loss in consumption: C falls to (1-b)*C\n",
    "# From Barro's data: b ranges from 0.15 to 0.64, mean ~0.29\n",
    "N_disaster = 100_000\n",
    "# Simulate b from a distribution that matches Barro's empirical moments\n",
    "b_samples = rng.uniform(0.15, 0.64, N_disaster)\n",
    "\n",
    "def disaster_moments(gamma, p, b_samples, mu_c, sigma_c, beta, phi):\n",
    "    \"\"\"Compute equity premium and riskfree rate under disaster risk (CRRA utility).\"\"\"\n",
    "    b = b_samples\n",
    "    one_minus_b = 1 - b\n",
    "\n",
    "    # E[(1-b)^{-gamma}] — disaster contribution to risk-free rate (lowers it via precaution)\n",
    "    E_M_disaster = np.mean(one_minus_b**(-gamma))\n",
    "\n",
    "    # Riskfree rate\n",
    "    # r_f = -log(beta) + gamma*mu_c - 0.5*gamma^2*sigma_c^2 - p*(E[(1-b)^{-gamma}] - 1)\n",
    "    rf = (-np.log(beta)\n",
    "          + gamma * mu_c\n",
    "          - 0.5 * gamma**2 * sigma_c**2\n",
    "          - p * (E_M_disaster - 1))\n",
    "\n",
    "    # Equity premium\n",
    "    # Normal-times: gamma*sigma_c^2 (same as CCAPM)\n",
    "    ep_normal = gamma * sigma_c**2\n",
    "\n",
    "    # Disaster term: p * E[(1-b)^{-gamma} * (1 - (1-b)^phi)]\n",
    "    # = p * (E[(1-b)^{-gamma}] - E[(1-b)^{phi-gamma}])\n",
    "    E_disaster_premium = np.mean(one_minus_b**(-gamma) * (1 - one_minus_b**phi))\n",
    "    ep_disaster = p * E_disaster_premium\n",
    "\n",
    "    ep_total = ep_normal + ep_disaster\n",
    "\n",
    "    return rf * 100, ep_total * 100, ep_normal * 100, ep_disaster * 100\n",
    "\n",
    "# Sweep over gamma\n",
    "gammas_barro = np.linspace(1.0, 8.0, 100)\n",
    "rf_barro = []\n",
    "ep_barro = []\n",
    "ep_normal_barro = []\n",
    "ep_disaster_barro = []\n",
    "\n",
    "for g in gammas_barro:\n",
    "    r, e, en, ed = disaster_moments(g, p_barro, b_samples, mu_c_barro,\n",
    "                                    sigma_c_barro, beta_barro, phi_barro)\n",
    "    rf_barro.append(r)\n",
    "    ep_barro.append(e)\n",
    "    ep_normal_barro.append(en)\n",
    "    ep_disaster_barro.append(ed)\n",
    "\n",
    "rf_barro = np.array(rf_barro)\n",
    "ep_barro = np.array(ep_barro)\n",
    "ep_normal_barro = np.array(ep_normal_barro)\n",
    "ep_disaster_barro = np.array(ep_disaster_barro)\n",
    "\n",
    "# Compare with no-disaster case\n",
    "ep_no_disaster = gammas_barro * sigma_c_barro**2 * 100\n",
    "rf_no_disaster = (-np.log(beta_barro) + gammas_barro * mu_c_barro\n",
    "                  - 0.5 * gammas_barro**2 * sigma_c_barro**2) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(gammas_barro, ep_barro, 'steelblue', lw=2.5, label='With disaster risk')\n",
    "ax.plot(gammas_barro, ep_no_disaster, 'grey', lw=2, ls='--', label='No disaster (CCAPM)')\n",
    "ax.axhline(6.0, color='tomato', lw=1.5, ls=':', label='Data target: 6%')\n",
    "ax.set_xlabel('Risk aversion $\\\\gamma$')\n",
    "ax.set_ylabel('Equity premium (% p.a.)')\n",
    "ax.set_title('Equity Premium\\nDisaster vs No-Disaster')\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_ylim(0, 15)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(gammas_barro, rf_barro, 'steelblue', lw=2.5, label='With disaster risk')\n",
    "ax2.plot(gammas_barro, rf_no_disaster, 'grey', lw=2, ls='--', label='No disaster')\n",
    "ax2.axhline(0.9, color='tomato', lw=1.5, ls=':', label='Data target: 0.9%')\n",
    "ax2.set_xlabel('Risk aversion $\\\\gamma$')\n",
    "ax2.set_ylabel('Riskfree rate (% p.a.)')\n",
    "ax2.set_title('Riskfree Rate\\nDisaster Lowers rf via Precaution')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.set_ylim(-5, 15)\n",
    "\n",
    "ax3 = axes[2]\n",
    "ax3.stackplot(gammas_barro, ep_normal_barro, ep_disaster_barro,\n",
    "              labels=['Normal-times EP', 'Disaster premium'],\n",
    "              colors=['#2980b9', '#e74c3c'], alpha=0.85)\n",
    "ax3.set_xlabel('Risk aversion $\\\\gamma$')\n",
    "ax3.set_ylabel('Equity premium components (% p.a.)')\n",
    "ax3.set_title('Decomposition of Equity Premium\\nDisaster Dominates')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.set_ylim(0, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find gamma that matches 6% EP\n",
    "from scipy.optimize import brentq\n",
    "def ep_minus_target(g):\n",
    "    _, e, _, _ = disaster_moments(g, p_barro, b_samples, mu_c_barro,\n",
    "                                  sigma_c_barro, beta_barro, phi_barro)\n",
    "    return e - 6.0\n",
    "\n",
    "g_star = brentq(ep_minus_target, 1.0, 8.0)\n",
    "rf_star, _, _, _ = disaster_moments(g_star, p_barro, b_samples, mu_c_barro,\n",
    "                                    sigma_c_barro, beta_barro, phi_barro)\n",
    "print(f\"γ required to match 6% equity premium: {g_star:.2f}\")\n",
    "print(f\"Implied riskfree rate at γ={g_star:.2f}: {rf_star:.2f}%  (data: ~0.9%)\")\n",
    "print(f\"\\nCompare: CCAPM requires γ ≈ {6.0 / (cov_g_r * 100):.0f} to match the same target.\")\n",
    "\n",
    "# Compute what fraction is disaster premium at gamma=g_star\n",
    "_, _, en_star, ed_star = disaster_moments(g_star, p_barro, b_samples, mu_c_barro,\n",
    "                                          sigma_c_barro, beta_barro, phi_barro)\n",
    "cov_g_r = 0.20 * sigma_c_barro * 0.165  # approximate from Week 2\n",
    "print(f\"Disaster premium share: {ed_star/6.0*100:.0f}% of total equity premium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 The Peso Problem and Identification\n",
    "\n",
    "The disaster risk model faces a fundamental empirical challenge: **disasters are rare by assumption**, so the postwar US data contains very few (arguably zero) clear consumption disasters. This raises the **peso problem**: how can we identify the probability and severity of events that have not occurred in our sample?\n",
    "\n",
    "Barro's answer is to use international cross-country data, where disasters (wars, financial crises, hyperinflations) are more frequent. But this introduces a different issue: the consumption and equity market data of other countries may not be comparable to the US.\n",
    "\n",
    "**Options market evidence**: If disaster risk is priced, options markets should reflect it. The implied volatility smile — the fact that out-of-the-money put options are more expensive than the Black-Scholes model predicts — can be interpreted as the market pricing the possibility of a large, sudden drop. Gabaix (2012) and others have formalised this connection.\n",
    "\n",
    "**Identification strategy**: Nakamura et al. (2013) use the long-run international data (35 countries, 1890–2006) and a Bayesian approach to estimate disaster parameters jointly with the preference parameters. They find $p \\approx 0.02$ and $E[b] \\approx 0.30$, consistent with Barro (2006).\n",
    "\n",
    "### 3.6 Long-Run Risks vs Disaster Risk: A Comparison\n",
    "\n",
    "Both models resolve the equity premium puzzle. Their key differences are:\n",
    "\n",
    "| Dimension | Long-Run Risks | Disaster Risk |\n",
    "|---|---|---|\n",
    "| **Mechanism** | Persistent small shocks priced by EZ agents | Rare large shocks priced by CRRA agents |\n",
    "| **Preferences needed** | EZ with $\\psi > 1$, $\\gamma \\approx 10$ | CRRA with $\\gamma \\approx 3$–5 |\n",
    "| **Empirical basis** | Predictability, vol clustering | International disaster data |\n",
    "| **Time-varying risk** | Yes (stochastic volatility) | Yes (time-varying $p$) |\n",
    "| **Options pricing** | Stochastic vol channel | Jump-risk channel |\n",
    "| **Criticism** | $\\psi > 1$ debated; long-run component barely visible | Peso problem; disasters may not recur |\n",
    "\n",
    "The field has not reached consensus. Both models are taken seriously in the literature, and ongoing work attempts to distinguish between them using options data, bond markets, and international cross-sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and compare consumption paths: LRR vs Disaster processes\n",
    "T_compare = 60   # 60 years — the postwar US sample\n",
    "n_sims    = 5000\n",
    "\n",
    "# LRR: annual g_c = mu_c + x_t + sigma_c * eps\n",
    "# Disaster: annual g_c = mu_c + sigma_c*eps with p=1.7% of a disaster b\n",
    "\n",
    "# Annual LRR consumption levels (set sigma_c same as Barro for fair comparison)\n",
    "mu_c_lrr   = 0.018\n",
    "sigma_c_lrr = 0.027\n",
    "rho_lrr    = 0.979\n",
    "phi_e_lrr  = 0.038\n",
    "\n",
    "# Draw LRR consumption paths\n",
    "x_lrr = np.zeros((T_compare, n_sims))\n",
    "g_lrr = np.zeros((T_compare, n_sims))\n",
    "x_lrr[0] = 0\n",
    "for t in range(T_compare - 1):\n",
    "    eps_t = rng.standard_normal(n_sims)\n",
    "    e_t   = rng.standard_normal(n_sims)\n",
    "    g_lrr[t]    = mu_c_lrr + x_lrr[t] + sigma_c_lrr * eps_t\n",
    "    x_lrr[t+1] = rho_lrr * x_lrr[t] + phi_e_lrr * sigma_c_lrr * e_t\n",
    "g_lrr[-1] = mu_c_lrr + x_lrr[-1] + sigma_c_lrr * rng.standard_normal(n_sims)\n",
    "\n",
    "# Cumulative consumption (log level, starting at 0)\n",
    "log_C_lrr = np.cumsum(g_lrr, axis=0)\n",
    "\n",
    "# Draw Disaster consumption paths\n",
    "g_disaster = rng.normal(mu_c_barro, sigma_c_barro, (T_compare, n_sims))\n",
    "disaster_indicator = rng.random((T_compare, n_sims)) < p_barro\n",
    "b_draw = rng.uniform(0.15, 0.64, (T_compare, n_sims))\n",
    "g_disaster += disaster_indicator * np.log(1 - b_draw)\n",
    "\n",
    "log_C_disaster = np.cumsum(g_disaster, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "years_c = np.arange(T_compare)\n",
    "\n",
    "# Panel 1: sample paths, 10 draws each\n",
    "ax = axes[0]\n",
    "for j in range(10):\n",
    "    ax.plot(years_c, log_C_lrr[:, j] * 100, color='steelblue', lw=0.8, alpha=0.6)\n",
    "for j in range(10):\n",
    "    ax.plot(years_c, log_C_disaster[:, j] * 100, color='tomato', lw=0.8, alpha=0.6,\n",
    "            ls='--')\n",
    "ax.plot([], [], 'steelblue', lw=1.5, label='Long-run risks')\n",
    "ax.plot([], [], 'tomato', lw=1.5, ls='--', label='Disaster risk')\n",
    "ax.set_xlabel('Year'); ax.set_ylabel('Log consumption (pp)')\n",
    "ax.set_title('Sample Paths (10 draws, 60 years)\\nDisaster vs Long-Run Risks')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 2: distribution of total 60-year log consumption change\n",
    "ax2 = axes[1]\n",
    "ax2.hist(log_C_lrr[-1] * 100, bins=60, density=True, alpha=0.6,\n",
    "         color='steelblue', label='LRR')\n",
    "ax2.hist(log_C_disaster[-1] * 100, bins=60, density=True, alpha=0.6,\n",
    "         color='tomato', label='Disaster')\n",
    "ax2.set_xlabel('60-year cumulative log consumption change (pp)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Distribution of Long-Run Outcomes\\nDisaster has heavier left tail')\n",
    "ax2.legend()\n",
    "\n",
    "# Panel 3: autocorrelation of consumption growth\n",
    "ax3 = axes[2]\n",
    "lags = np.arange(1, 11)\n",
    "ac_lrr      = [np.corrcoef(g_lrr[:-k].flatten(), g_lrr[k:].flatten())[0,1] for k in lags]\n",
    "ac_disaster = [np.corrcoef(g_disaster[:-k].flatten(), g_disaster[k:].flatten())[0,1] for k in lags]\n",
    "ax3.bar(lags - 0.2, ac_lrr, 0.35, label='LRR', color='steelblue', alpha=0.85)\n",
    "ax3.bar(lags + 0.2, ac_disaster, 0.35, label='Disaster', color='tomato', alpha=0.85)\n",
    "ax3.axhline(0, color='black', lw=0.8)\n",
    "ax3.set_xlabel('Lag (years)'); ax3.set_ylabel('Autocorrelation')\n",
    "ax3.set_title('Autocorrelation of Consumption Growth\\nLRR has persistent structure')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key distinction: the LRR model generates persistent autocorrelation in consumption growth.\")\n",
    "print(\"The disaster model generates a heavier left tail but little autocorrelation.\")\n",
    "print(\"These different predictions can (in principle) be tested empirically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C — The Arbitrage Pricing Theory and Factor Models\n",
    "\n",
    "---\n",
    "\n",
    "## 4. From Equilibrium to Empirical Factors\n",
    "\n",
    "Parts A and B derived specific equilibrium models of the SDF. These models make strong structural predictions — both about the level of expected returns (equity premium, riskfree rate) and about the cross-sectional variation (which assets have high expected returns and why).\n",
    "\n",
    "However, the equilibrium models require strong assumptions about preferences, the consumption process, and market structure. An alternative, more agnostic approach is to characterise the SDF empirically — to ask: what observable factors explain the cross-sectional variation in expected returns, without requiring a full structural model?\n",
    "\n",
    "This is the empirical asset pricing programme, which begins with the Arbitrage Pricing Theory.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. The Arbitrage Pricing Theory (Ross, 1976)\n",
    "\n",
    "### 5.1 The Factor Structure\n",
    "\n",
    "**Assumption**: Returns are generated by a $K$-factor model:\n",
    "$$\n",
    "R^i = E[R^i] + \\sum_{k=1}^{K} \\beta_{ik} f_k + \\varepsilon^i,\n",
    "$$\n",
    "where:\n",
    "- $f_k$ are zero-mean common factors, $\\text{Cov}(f_k, \\varepsilon^i) = 0$ for all $i, k$.\n",
    "- $\\varepsilon^i$ is idiosyncratic noise: $\\text{Cov}(\\varepsilon^i, \\varepsilon^j) = 0$ for $i \\neq j$ (approximate factor structure).\n",
    "- $\\beta_{ik}$ is asset $i$'s loading (\"beta\") on factor $k$.\n",
    "\n",
    "This is a purely statistical assumption about the return covariance structure. No utility functions, no representative agents.\n",
    "\n",
    "### 5.2 The APT Pricing Result\n",
    "\n",
    "**Theorem (Ross, 1976)**: In an economy with a factor structure as above, if there are no approximate arbitrage opportunities (formal definition below), then there exist constants $\\lambda_0, \\lambda_1, \\ldots, \\lambda_K$ such that:\n",
    "$$\n",
    "\\boxed{E[R^i] = \\lambda_0 + \\sum_{k=1}^{K} \\beta_{ik} \\lambda_k,}\n",
    "$$\n",
    "where $\\lambda_0 = R_f$ (the riskfree rate) and $\\lambda_k$ is the **factor risk premium** for factor $k$.\n",
    "\n",
    "**Proof sketch (law of one price argument)**:\n",
    "Consider a portfolio $w$ of $N$ assets that is:\n",
    "1. Zero-cost: $\\mathbf{w} \\cdot \\mathbf{1} = 0$.\n",
    "2. Zero-beta on all factors: $\\mathbf{w} \\cdot \\boldsymbol{\\beta}_k = 0$ for all $k$.\n",
    "3. Well-diversified: the idiosyncratic component $\\mathbf{w} \\cdot \\boldsymbol{\\varepsilon} \\to 0$ as $N \\to \\infty$.\n",
    "\n",
    "Such a portfolio has zero risk (as $N \\to \\infty$) but may have non-zero expected return $\\mathbf{w} \\cdot E[\\mathbf{R}]$. No-arbitrage requires this expected return to be zero. By the duality of linear programming, this implies that $E[\\mathbf{R}]$ lies in the column space of $[\\mathbf{1}, \\boldsymbol{\\beta}_1, \\ldots, \\boldsymbol{\\beta}_K]$ — which is exactly the APT equation.\n",
    "\n",
    "### 5.3 APT vs CAPM\n",
    "\n",
    "The APT is more general than the CAPM in one sense and less precise in another:\n",
    "\n",
    "- **More general**: The APT allows multiple factors and does not require a representative agent or market portfolio identification.\n",
    "- **Less precise**: The APT does not identify the factors. It says expected returns are linear in betas, but does not say what the betas are. The CAPM identifies the factor as the market portfolio.\n",
    "- **APT is approximate**: The no-arbitrage argument requires idiosyncratic risks to vanish — it works for well-diversified portfolios but may have large deviations for individual stocks.\n",
    "\n",
    "**Connection to the SDF**: The APT factor structure implies that the SDF can be written as a linear function of the factors:\n",
    "$$\n",
    "M = a_0 + \\sum_{k=1}^K a_k f_k.\n",
    "$$\n",
    "Each factor that appears in $M$ carries a risk premium. The APT is thus a model of $M$ — specifically, a multi-factor linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the APT: show that diversification eliminates idiosyncratic risk\n",
    "# but beta risk remains even in large portfolios\n",
    "\n",
    "N_assets_apt = 200   # total assets\n",
    "K_factors    = 3     # number of factors\n",
    "T_apt        = 120   # months (10 years)\n",
    "\n",
    "# Factor risk premia\n",
    "lambda_apt = np.array([0.005, 0.003, 0.004])  # monthly\n",
    "Rf_apt = 0.001\n",
    "\n",
    "# Factor covariance matrix (diagonal for simplicity)\n",
    "factor_vols = np.array([0.045, 0.035, 0.030])  # monthly factor vols\n",
    "Sigma_f = np.diag(factor_vols**2)\n",
    "\n",
    "# Draw betas from uniform distribution\n",
    "betas_apt = rng.uniform(0.2, 1.8, (N_assets_apt, K_factors))\n",
    "\n",
    "# True expected excess returns\n",
    "E_excess_apt = betas_apt @ lambda_apt\n",
    "\n",
    "# Simulate factor realisations\n",
    "f_sim = rng.normal(0, factor_vols, (T_apt, K_factors))\n",
    "\n",
    "# Idiosyncratic noise (all assets, each period)\n",
    "sigma_eps_apt = rng.uniform(0.06, 0.15, N_assets_apt)  # idiosyncratic vol (monthly)\n",
    "eps_sim = rng.normal(0, 1, (T_apt, N_assets_apt)) * sigma_eps_apt[None, :]\n",
    "\n",
    "# Realised returns\n",
    "R_sim = (E_excess_apt[None, :]   # expected excess return\n",
    "         + f_sim @ betas_apt.T   # factor component\n",
    "         + eps_sim)              # idiosyncratic\n",
    "\n",
    "# Build equal-weighted portfolios of size n and compute vol decomposition\n",
    "portfolio_sizes = [1, 2, 5, 10, 20, 50, 100, 200]\n",
    "systematic_var  = []\n",
    "idiosyncratic_var = []\n",
    "total_var       = []\n",
    "\n",
    "for n in portfolio_sizes:\n",
    "    # Take first n assets, equal weight\n",
    "    w = np.ones(n) / n\n",
    "    port_ret = R_sim[:, :n] @ w  # (T,)\n",
    "\n",
    "    # Systematic component: factor exposure of portfolio\n",
    "    beta_port = betas_apt[:n].T @ w  # (K,)\n",
    "    sys_var = beta_port @ Sigma_f @ beta_port\n",
    "\n",
    "    # Idiosyncratic component: shrinks with 1/n\n",
    "    idio_var = np.sum(w**2 * sigma_eps_apt[:n]**2)\n",
    "\n",
    "    systematic_var.append(sys_var)\n",
    "    idiosyncratic_var.append(idio_var)\n",
    "    total_var.append(port_ret.var())\n",
    "\n",
    "systematic_var   = np.array(systematic_var)\n",
    "idiosyncratic_var = np.array(idiosyncratic_var)\n",
    "total_var        = np.array(total_var)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(portfolio_sizes, systematic_var * 100**2, 'steelblue', lw=2, marker='o', ms=6,\n",
    "        label='Systematic variance')\n",
    "ax.plot(portfolio_sizes, idiosyncratic_var * 100**2, 'tomato', lw=2, marker='s', ms=6,\n",
    "        label='Idiosyncratic variance')\n",
    "ax.plot(portfolio_sizes, total_var * 100**2, 'k--', lw=1.5, marker='^', ms=5,\n",
    "        label='Total portfolio variance')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Number of assets in portfolio')\n",
    "ax.set_ylabel('Variance (×10⁴, monthly)')\n",
    "ax.set_title('Variance Decomposition as Portfolio Size Grows\\nIdiosyncratic risk diversifies away; systematic does not')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 2: APT cross-sectional regression\n",
    "# True vs estimated expected returns using known betas\n",
    "# Fama-MacBeth: estimate betas in first half, price factors in second half\n",
    "T_half = T_apt // 2\n",
    "\n",
    "# Estimated betas (OLS in first half)\n",
    "X_ts_apt = np.column_stack([np.ones(T_half), f_sim[:T_half]])\n",
    "betas_estimated_apt = np.empty((N_assets_apt, K_factors))\n",
    "for i in range(N_assets_apt):\n",
    "    coef = np.linalg.lstsq(X_ts_apt, R_sim[:T_half, i], rcond=None)[0]\n",
    "    betas_estimated_apt[i] = coef[1:]\n",
    "\n",
    "# Cross-sectional regression in second half\n",
    "E_R_second = R_sim[T_half:].mean(axis=0)  # realised mean returns\n",
    "X_cs_apt = np.column_stack([np.ones(N_assets_apt), betas_estimated_apt])\n",
    "lambda_hat = np.linalg.lstsq(X_cs_apt, E_R_second, rcond=None)[0]\n",
    "\n",
    "E_R_predicted = X_cs_apt @ lambda_hat\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(E_R_predicted * 1200, E_R_second * 1200,\n",
    "            alpha=0.5, s=20, color='steelblue', edgecolors='none')\n",
    "lims = [min(E_R_predicted.min(), E_R_second.min()) * 1200 - 0.5,\n",
    "        max(E_R_predicted.max(), E_R_second.max()) * 1200 + 0.5]\n",
    "ax2.plot(lims, lims, 'k--', lw=1.5, label='45-degree line')\n",
    "ax2.set_xlabel('APT-predicted $E[R^i]$ (% p.a.)')\n",
    "ax2.set_ylabel('Realised mean return (% p.a.)')\n",
    "ax2.set_title(f'APT Cross-Sectional Fit\\n($K={K_factors}$ factors, $N={N_assets_apt}$ assets)')\n",
    "ax2.legend()\n",
    "\n",
    "# R-squared\n",
    "ss_res = np.sum((E_R_second - E_R_predicted)**2)\n",
    "ss_tot = np.sum((E_R_second - E_R_second.mean())**2)\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "ax2.text(0.05, 0.92, f'$R^2 = {r2:.2f}$', transform=ax2.transAxes, fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated factor risk premia (annualised, %):\\n\"\n",
    "      f\"  λ₀={lambda_hat[0]*1200:.2f}  λ₁={lambda_hat[1]*1200:.2f}  \"\n",
    "      f\"λ₂={lambda_hat[2]*1200:.2f}  λ₃={lambda_hat[3]*1200:.2f}\")\n",
    "print(f\"True factor risk premia (annualised, %):      \"\n",
    "      f\"  λ₀={Rf_apt*1200:.2f}  λ₁={lambda_apt[0]*1200:.2f}  \"\n",
    "      f\"λ₂={lambda_apt[1]*1200:.2f}  λ₃={lambda_apt[2]*1200:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. The Fama-French Factor Models\n",
    "\n",
    "### 6.1 From Theory to Empirical Factors\n",
    "\n",
    "The APT tells us that expected returns are linear in factor betas. It does not tell us what the factors are. The empirical programme of Fama and French identifies factors by sorting stocks on observable firm characteristics and constructing zero-cost long-short portfolios.\n",
    "\n",
    "**Key idea**: If a characteristic predicts expected returns in the cross-section — even after controlling for market beta — then it reveals information about an omitted factor in the SDF. The characteristic-sorted portfolio is a direct estimate of the priced factor.\n",
    "\n",
    "### 6.2 The Fama-French Three-Factor Model (1993)\n",
    "\n",
    "Fama and French (1993) documented that the CAPM leaves large, systematic residuals in the cross-section. In particular:\n",
    "- **Small stocks** earn higher returns than large stocks, after controlling for beta.\n",
    "- **Value stocks** (high book-to-market ratio, $B/M$) earn higher returns than growth stocks (low $B/M$), after controlling for beta.\n",
    "\n",
    "They proposed two additional factors to capture these premia:\n",
    "\n",
    "- **SMB** (Small Minus Big): The return on a portfolio long small-cap stocks and short large-cap stocks. Compensates for the size premium.\n",
    "- **HML** (High Minus Low): The return on a portfolio long high-$B/M$ (value) stocks and short low-$B/M$ (growth) stocks. Compensates for the value premium.\n",
    "\n",
    "The **Fama-French three-factor model**:\n",
    "$$\n",
    "\\boxed{E[R^i] - R_f = \\beta^{\\text{MKT}}_i \\lambda_{\\text{MKT}} + \\beta^{\\text{SMB}}_i \\lambda_{\\text{SMB}} + \\beta^{\\text{HML}}_i \\lambda_{\\text{HML}}.}\n",
    "$$\n",
    "\n",
    "### 6.3 Construction of SMB and HML\n",
    "\n",
    "**SMB** is constructed by:\n",
    "1. At the end of June each year, rank all NYSE stocks by market capitalisation and split at the median.\n",
    "2. Independently rank all NYSE stocks by $B/M$ and split into three groups (bottom 30%, middle 40%, top 30%).\n",
    "3. Form six intersecting portfolios: SG (small growth), SN (small neutral), SV (small value), BG, BN, BV.\n",
    "4. $\\text{SMB} = \\frac{1}{3}(\\text{SG} + \\text{SN} + \\text{SV}) - \\frac{1}{3}(\\text{BG} + \\text{BN} + \\text{BV})$.\n",
    "\n",
    "**HML** is constructed by:\n",
    "$$\n",
    "\\text{HML} = \\frac{1}{2}(\\text{SV} + \\text{BV}) - \\frac{1}{2}(\\text{SG} + \\text{BG}).\n",
    "$$\n",
    "\n",
    "The construction ensures that each factor is independent of the other and of the market. Portfolios are rebalanced annually.\n",
    "\n",
    "### 6.4 The Fama-French Five-Factor Model (2015)\n",
    "\n",
    "Fama and French (2015) augmented the three-factor model with two additional factors motivated by the dividend discount model. If the price equals the present value of future dividends, then controlling for price (i.e., $B/M$), stocks with higher expected profitability or lower expected investment should have higher expected returns.\n",
    "\n",
    "- **RMW** (Robust Minus Weak): Long firms with high operating profitability, short firms with low profitability.\n",
    "- **CMA** (Conservative Minus Aggressive): Long firms with low asset growth (conservative investment), short firms with high asset growth.\n",
    "\n",
    "The **five-factor model**:\n",
    "$$\n",
    "E[R^i] - R_f = \\beta^{\\text{MKT}}_i \\lambda_{\\text{MKT}} + \\beta^{\\text{SMB}}_i \\lambda_{\\text{SMB}} + \\beta^{\\text{HML}}_i \\lambda_{\\text{HML}} + \\beta^{\\text{RMW}}_i \\lambda_{\\text{RMW}} + \\beta^{\\text{CMA}}_i \\lambda_{\\text{CMA}}.\n",
    "$$\n",
    "\n",
    "**Important caveat**: Adding factors reduces the pricing errors by construction (more parameters), but raises the question of whether the new factors are genuinely priced risk factors or spurious overfitting. We return to this in the section on model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Fama-French factor construction and time-series properties\n",
    "# Use simulated data with known factor structure\n",
    "\n",
    "# We simulate monthly factor returns calibrated to FF historical moments\n",
    "# US data (1963-2023 approximate): annualised means and vols\n",
    "\n",
    "# Factor means (annual %) and vols\n",
    "factor_means_ann = np.array([8.5,  2.8,  3.8,  3.3, 2.3])   # MKT, SMB, HML, RMW, CMA\n",
    "factor_vols_ann  = np.array([15.5, 10.5, 11.2, 8.0, 6.5])   # annualised vols\n",
    "factor_names     = ['MKT', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Correlation matrix (approximate from data)\n",
    "corr_ff = np.array([\n",
    "    [ 1.00,  0.27, -0.37, -0.26, -0.39],\n",
    "    [ 0.27,  1.00, -0.13, -0.38, -0.13],\n",
    "    [-0.37, -0.13,  1.00,  0.10,  0.68],\n",
    "    [-0.26, -0.38,  0.10,  1.00,  0.07],\n",
    "    [-0.39, -0.13,  0.68,  0.07,  1.00],\n",
    "])\n",
    "\n",
    "# Monthly parameters\n",
    "factor_means_m = factor_means_ann / 100 / 12\n",
    "factor_vols_m  = factor_vols_ann  / 100 / np.sqrt(12)\n",
    "\n",
    "Sigma_ff = np.outer(factor_vols_m, factor_vols_m) * corr_ff\n",
    "\n",
    "T_ff = 12 * 60   # 60 years monthly\n",
    "\n",
    "# Draw factor returns\n",
    "L = cholesky(Sigma_ff, lower=True)\n",
    "z = rng.standard_normal((T_ff, 5))\n",
    "F_ff = z @ L.T + factor_means_m[None, :]  # (T, 5) factor returns\n",
    "\n",
    "# Cumulative factor returns\n",
    "cum_ff = np.cumsum(F_ff, axis=0)\n",
    "years_ff = np.arange(T_ff) / 12\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "\n",
    "colors_ff = ['#2c3e50', '#e74c3c', '#e67e22', '#27ae60', '#8e44ad']\n",
    "\n",
    "for i, (name, color) in enumerate(zip(factor_names, colors_ff)):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.plot(years_ff, cum_ff[:, i] * 100, color=color, lw=1.2)\n",
    "    ax.axhline(0, color='black', lw=0.6, ls='--')\n",
    "    mean_ann = F_ff[:, i].mean() * 1200\n",
    "    vol_ann  = F_ff[:, i].std() * 100 * np.sqrt(12)\n",
    "    sr       = mean_ann / vol_ann\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Cumulative return (pp)')\n",
    "    ax.set_title(f'{name} Factor\\n$E[R]$={mean_ann:.1f}%, $\\\\sigma$={vol_ann:.1f}%, SR={sr:.2f}')\n",
    "\n",
    "# Last panel: correlation heatmap\n",
    "ax_corr = axes[1, 2]\n",
    "corr_sim = np.corrcoef(F_ff.T)\n",
    "im = ax_corr.imshow(corr_sim, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax_corr.set_xticks(range(5)); ax_corr.set_yticks(range(5))\n",
    "ax_corr.set_xticklabels(factor_names); ax_corr.set_yticklabels(factor_names)\n",
    "plt.colorbar(im, ax=ax_corr)\n",
    "for ii in range(5):\n",
    "    for jj in range(5):\n",
    "        ax_corr.text(jj, ii, f'{corr_sim[ii, jj]:.2f}', ha='center', va='center',\n",
    "                     fontsize=9, color='black' if abs(corr_sim[ii, jj]) < 0.5 else 'white')\n",
    "ax_corr.set_title('Simulated Factor Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Fama-French Five Factors: Simulated Time Series', fontsize=13,\n",
    "             fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: HML and CMA are highly correlated (~0.68 in data).\")\n",
    "print(\"This raises the question of whether both are separately necessary.\")\n",
    "print(\"Indeed, HML becomes redundant in FF5 for many test portfolios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Evaluating Factor Models: The GRS Test and Pricing Errors\n",
    "\n",
    "### 7.1 Time-Series Regressions and Alpha\n",
    "\n",
    "The standard test of a factor model is a time-series regression of each test asset's excess return on the proposed factors:\n",
    "$$\n",
    "R^i_t - R_{f,t} = \\alpha_i + \\sum_{k=1}^{K} \\beta_{ik} F_{k,t} + \\varepsilon_{i,t}.\n",
    "$$\n",
    "If the model is correctly specified, **all $\\alpha_i$ should be zero**. A non-zero $\\alpha_i$ (\"alpha\" or \"pricing error\") means the model fails to explain the average return of asset $i$.\n",
    "\n",
    "**Statistical test (Gibbons-Ross-Shanken, 1989)**: The GRS statistic tests $H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_N = 0$ jointly:\n",
    "$$\n",
    "\\text{GRS} = \\frac{T(T - N - K)}{N(T - K - 1)} \\cdot \\hat{\\boldsymbol{\\alpha}}' \\hat{\\boldsymbol{\\Sigma}}^{-1} \\hat{\\boldsymbol{\\alpha}} \\cdot \\left(1 + \\bar{\\mathbf{f}}' \\hat{\\boldsymbol{\\Omega}}^{-1} \\bar{\\mathbf{f}}\\right)^{-1} \\sim F_{N, T-N-K},\n",
    "$$\n",
    "where:\n",
    "- $\\hat{\\boldsymbol{\\alpha}}$ is the vector of estimated intercepts.\n",
    "- $\\hat{\\boldsymbol{\\Sigma}}$ is the $N \\times N$ covariance matrix of residuals.\n",
    "- $\\bar{\\mathbf{f}}$ is the vector of sample mean factor returns.\n",
    "- $\\hat{\\boldsymbol{\\Omega}}$ is the $K \\times K$ factor covariance matrix.\n",
    "\n",
    "The GRS statistic has a geometric interpretation: it measures the improvement in the maximum Sharpe ratio achievable by adding the test assets to the factor portfolio. If all alphas are zero, adding the test assets does not improve the maximum Sharpe ratio.\n",
    "\n",
    "### 7.2 The Sharpe Ratio Interpretation\n",
    "\n",
    "Let $SR_f$ be the Sharpe ratio of the tangency portfolio formed from the $K$ factors alone, and $SR_{f+}$ be the Sharpe ratio of the tangency portfolio formed from the factors plus all $N$ test assets. Then:\n",
    "$$\n",
    "SR_{f+}^2 = SR_f^2 + \\frac{\\hat{\\boldsymbol{\\alpha}}' \\hat{\\boldsymbol{\\Sigma}}^{-1} \\hat{\\boldsymbol{\\alpha}}}{1}.\n",
    "$$\n",
    "The GRS statistic is proportional to $SR_{f+}^2 - SR_f^2$. A factor model passes the GRS test when including the test assets does not improve the Sharpe ratio — meaning the factors already span the efficient frontier.\n",
    "\n",
    "### 7.3 The HJ Distance\n",
    "\n",
    "An alternative to the GRS test is the **Hansen-Jagannathan (HJ) distance**, introduced in Week 1. For a candidate SDF $M^\\theta$ parameterised by $\\theta$, the HJ distance is:\n",
    "$$\n",
    "\\delta(\\theta) = \\min_{\\theta} \\sqrt{E\\left[(M^\\theta - M^*)^2\\right]},\n",
    "$$\n",
    "where $M^*$ is the minimum-variance SDF that prices the test assets exactly. The HJ distance can be estimated as:\n",
    "$$\n",
    "\\hat{\\delta}^2(\\theta) = (\\mathbf{1} - E[M^\\theta \\mathbf{R}])' \\left(E[\\mathbf{R}\\mathbf{R}']\\right)^{-1} (\\mathbf{1} - E[M^\\theta \\mathbf{R}]).\n",
    "$$\n",
    "It is interpretable in the same units as the SDF (ratio of marginal utilities), making it easier to compare across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the GRS test and compare CAPM vs FF3 vs FF5\n",
    "# Use simulated 25 size-value sorted portfolios with a known 5-factor structure\n",
    "\n",
    "T_grs = 600   # 50 years monthly\n",
    "N_grs = 25    # 25 test portfolios (5x5 size-value sorts)\n",
    "\n",
    "# Simulate: true model is FF5 (all 5 factors matter)\n",
    "# CAPM will have large alphas; FF3 smaller; FF5 near zero\n",
    "\n",
    "# Draw factor returns (same as above, but fresh draw)\n",
    "z2 = rng.standard_normal((T_grs, 5))\n",
    "F_grs = z2 @ L.T + factor_means_m[None, :]   # (T_grs, 5)\n",
    "\n",
    "# Assign betas to 25 portfolios (structured by size and value)\n",
    "# Size: rows (1=small, 5=large); Value: cols (1=growth, 5=value)\n",
    "size_idx  = np.repeat(np.arange(5), 5)   # 0..4\n",
    "value_idx = np.tile(np.arange(5), 5)     # 0..4\n",
    "\n",
    "# MKT beta close to 1 for all; SMB decreases with size; HML increases with value\n",
    "beta_MKT = 0.9 + 0.05 * rng.standard_normal(N_grs)\n",
    "beta_SMB = 1.0 - 0.25 * size_idx  + 0.05 * rng.standard_normal(N_grs)\n",
    "beta_HML = -0.4 + 0.20 * value_idx + 0.05 * rng.standard_normal(N_grs)\n",
    "beta_RMW = 0.1 + 0.05 * rng.standard_normal(N_grs)   # small RMW exposure\n",
    "beta_CMA = 0.05 + 0.04 * rng.standard_normal(N_grs)  # small CMA exposure\n",
    "\n",
    "betas_grs = np.column_stack([beta_MKT, beta_SMB, beta_HML, beta_RMW, beta_CMA])\n",
    "\n",
    "# True expected excess returns from FF5 model\n",
    "E_excess_grs = betas_grs @ factor_means_m  # (N_grs,)\n",
    "\n",
    "# Simulate portfolio excess returns\n",
    "sigma_eps_grs = 0.02  # monthly idiosyncratic vol (well-diversified portfolios)\n",
    "eps_grs = rng.normal(0, sigma_eps_grs, (T_grs, N_grs))\n",
    "R_grs = E_excess_grs[None, :] + F_grs @ betas_grs.T + eps_grs  # (T, N)\n",
    "\n",
    "def run_grs_test(R_test, F_factors):\n",
    "    \"\"\"Run GRS test of H0: all alphas = 0 in time-series regression on F_factors.\"\"\"\n",
    "    T, N = R_test.shape\n",
    "    K = F_factors.shape[1]\n",
    "\n",
    "    # Add intercept to factor matrix\n",
    "    X = np.column_stack([np.ones(T), F_factors])\n",
    "\n",
    "    # OLS regression for each portfolio\n",
    "    coefs = np.linalg.lstsq(X, R_test, rcond=None)[0]  # (1+K, N)\n",
    "    alphas = coefs[0]     # (N,)\n",
    "    resids = R_test - X @ coefs  # (T, N)\n",
    "\n",
    "    # Residual covariance matrix\n",
    "    Sigma_hat = resids.T @ resids / (T - K - 1)  # (N, N)\n",
    "\n",
    "    # Factor moments\n",
    "    f_bar = F_factors.mean(axis=0)   # (K,)\n",
    "    Omega_hat = np.cov(F_factors.T)  # (K, K)\n",
    "\n",
    "    # GRS statistic\n",
    "    Sigma_inv = np.linalg.inv(Sigma_hat)\n",
    "    if K == 1:\n",
    "        Omega_inv_scalar = 1.0 / Omega_hat\n",
    "        scaling = 1 + f_bar[0] * Omega_inv_scalar * f_bar[0]\n",
    "    else:\n",
    "        scaling = 1 + f_bar @ np.linalg.inv(Omega_hat) @ f_bar\n",
    "\n",
    "    grs_stat = (T * (T - N - K)) / (N * (T - K - 1)) * (alphas @ Sigma_inv @ alphas) / scaling\n",
    "\n",
    "    # Degrees of freedom\n",
    "    from scipy.stats import f as f_dist\n",
    "    p_value = 1 - f_dist.cdf(grs_stat, N, T - N - K)\n",
    "\n",
    "    # Mean absolute alpha and cross-sectional R2\n",
    "    mean_abs_alpha = np.abs(alphas).mean() * 1200  # annualised %\n",
    "\n",
    "    return {\n",
    "        'alphas': alphas,\n",
    "        'grs_stat': grs_stat,\n",
    "        'p_value': p_value,\n",
    "        'mean_abs_alpha': mean_abs_alpha,\n",
    "        'Sigma': Sigma_hat,\n",
    "    }\n",
    "\n",
    "# Test three models\n",
    "res_capm = run_grs_test(R_grs, F_grs[:, [0]])        # CAPM: MKT only\n",
    "res_ff3  = run_grs_test(R_grs, F_grs[:, [0,1,2]])    # FF3: MKT, SMB, HML\n",
    "res_ff5  = run_grs_test(R_grs, F_grs)                 # FF5: all five\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"GRS Test Results — {N_grs} portfolios, {T_grs} months\")\n",
    "print(\"=\" * 65)\n",
    "for name, res in [('CAPM', res_capm), ('FF3', res_ff3), ('FF5', res_ff5)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  GRS statistic:        {res['grs_stat']:.3f}\")\n",
    "    print(f\"  p-value:              {res['p_value']:.4f}\")\n",
    "    print(f\"  Mean |alpha| (% p.a.): {res['mean_abs_alpha']:.3f}\")\n",
    "    print(f\"  Reject H0 (α=5%)?     {'YES' if res['p_value'] < 0.05 else 'NO'}\")\n",
    "\n",
    "# Visualise alphas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "labels_25 = [f'({s+1},{v+1})' for s in range(5) for v in range(5)]\n",
    "x25 = np.arange(N_grs)\n",
    "\n",
    "for ax, (name, res) in zip(axes, [('CAPM', res_capm), ('FF3', res_ff3), ('FF5', res_ff5)]):\n",
    "    alphas_ann = res['alphas'] * 1200\n",
    "    se_alpha   = np.sqrt(np.diag(res['Sigma'])) / np.sqrt(T_grs) * 1200\n",
    "    colors_alpha = ['#e74c3c' if a > 0 else '#2980b9' for a in alphas_ann]\n",
    "    ax.bar(x25, alphas_ann, color=colors_alpha, alpha=0.75, edgecolor='black', lw=0.3)\n",
    "    ax.errorbar(x25, alphas_ann, yerr=2*se_alpha, fmt='none', color='black', capsize=2, lw=0.8)\n",
    "    ax.axhline(0, color='black', lw=1)\n",
    "    ax.set_xlabel('Portfolio (size, value)')\n",
    "    ax.set_ylabel('Alpha (% p.a.)')\n",
    "    ax.set_title(f'{name}\\nGRS={res[\"grs_stat\"]:.2f}, p={res[\"p_value\"]:.3f}, '\n",
    "                 f'|ᾱ|={res[\"mean_abs_alpha\"]:.2f}%')\n",
    "    ax.set_xticks(x25[::5])\n",
    "    ax.set_xticklabels(labels_25[::5], fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Pricing Errors (Alphas) for 25 Size-Value Portfolios\\n'\n",
    "             'Red = positive alpha, Blue = negative alpha; error bars = ±2 s.e.',\n",
    "             fontsize=12, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. The Factor Zoo and the Multiple Testing Problem\n",
    "\n",
    "### 8.1 The Proliferation of Factors\n",
    "\n",
    "The empirical asset pricing literature has produced a vast number of proposed factors since Fama and French (1993). Harvey, Liu, and Zhu (2016) documented over 316 factors published in leading journals by 2014 — a number that has grown substantially since. These factors include momentum, quality, low volatility, accruals, asset growth, earnings surprises, and dozens more.\n",
    "\n",
    "Cochrane (2011) referred to this as the **factor zoo**: a proliferation of empirical factors with little theoretical foundation, many of which may be the result of data mining rather than genuine risk premia.\n",
    "\n",
    "### 8.2 The Multiple Testing Problem\n",
    "\n",
    "The standard criterion for declaring a new factor significant is a $t$-statistic above 2.0 (corresponding to a $p$-value below 5%). But if 316 factors are tested independently, the expected number of false positives under the null of no true factor is:\n",
    "$$\n",
    "E[\\text{false positives}] = 316 \\times 0.05 = 15.8.\n",
    "$$\n",
    "This is the **multiple testing problem**: the more factors we test, the more likely we are to find spurious ones by chance.\n",
    "\n",
    "**Harvey, Liu, and Zhu (2016) proposal**: The threshold for claiming a new factor is significant should account for the number of tests conducted. Using a Bayesian framework, they argue that the appropriate threshold for the $t$-statistic of a new factor should be approximately 3.0 (rather than 2.0) to maintain a false discovery rate below 5%.\n",
    "\n",
    "**Implications**: Many published factors with $t$-statistics between 2.0 and 3.0 may be false discoveries. The expected number of true factors is far smaller than 316.\n",
    "\n",
    "### 8.3 Out-of-Sample Performance\n",
    "\n",
    "A related concern is **look-ahead bias**: factors identified using the full history of data cannot be tested out-of-sample on the same data. McLean and Pontiff (2016) showed that anomalies identified in academic research have significantly weaker performance in the post-publication period — consistent with a combination of overfitting and arbitrage by informed investors who trade on published findings.\n",
    "\n",
    "The decay in factor premia after publication has two non-exclusive explanations:\n",
    "1. **Statistical overfitting**: The in-sample premium was partly spurious; the out-of-sample premium is lower.\n",
    "2. **Arbitrage**: Informed investors trade the anomaly, reducing the mispricing. If the premium is real, it should persist (risk-based); if it is mispricing, it should decay.\n",
    "\n",
    "### 8.4 What Constitutes a Valid Factor?\n",
    "\n",
    "Cochrane (2011) argued that identifying which factors are genuine requires:\n",
    "\n",
    "1. **Economic theory**: The factor should have a clear theoretical motivation as a priced risk or an SDF ingredient.\n",
    "2. **Out-of-sample evidence**: The factor should work in international data, in different time periods, and in asset classes other than equities.\n",
    "3. **Spanning tests**: The factor should not be spanned by existing factors — it adds information beyond what is already in the model.\n",
    "4. **Conservative $t$-statistics**: The $t$-statistic should exceed 3.0 (Harvey et al.) or be robust to reasonable resampling.\n",
    "\n",
    "By these criteria, only a handful of factors survive: the market, value, profitability, and momentum have the strongest evidence. Investment has mixed evidence. Most of the remaining 300+ factors are unlikely to be genuine priced risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the multiple testing problem\n",
    "# Show: how many factors appear significant purely by chance?\n",
    "\n",
    "n_factors_tested = 300   # number of candidate factors tested\n",
    "T_mt = 600               # 50 years monthly observations\n",
    "n_simulations = 2000     # Monte Carlo replications\n",
    "\n",
    "# Under H0: ALL factors are null (no true risk premium)\n",
    "# Generate factor return series: iid N(0, sigma_factor)\n",
    "sigma_factor_mt = 0.035  # monthly factor vol\n",
    "\n",
    "# For each simulation, count how many factors have |t| > 2.0 and |t| > 3.0\n",
    "count_t20 = np.empty(n_simulations)  # |t| > 2.0\n",
    "count_t30 = np.empty(n_simulations)  # |t| > 3.0\n",
    "\n",
    "for sim in range(n_simulations):\n",
    "    # Draw n_factors_tested factors of length T_mt\n",
    "    F_null = rng.normal(0, sigma_factor_mt, (T_mt, n_factors_tested))\n",
    "    # t-statistic for each factor: t = sqrt(T) * mean(F) / std(F)\n",
    "    t_stats = np.sqrt(T_mt) * F_null.mean(axis=0) / F_null.std(axis=0)\n",
    "    count_t20[sim] = np.sum(np.abs(t_stats) > 2.0)\n",
    "    count_t30[sim] = np.sum(np.abs(t_stats) > 3.0)\n",
    "\n",
    "# Under H0: expected false positives\n",
    "from scipy.stats import norm as norm_stats\n",
    "expected_t20 = n_factors_tested * 2 * (1 - norm_stats.cdf(2.0))\n",
    "expected_t30 = n_factors_tested * 2 * (1 - norm_stats.cdf(3.0))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(count_t20, bins=40, density=True, alpha=0.7, color='tomato',\n",
    "        label=f'|t|>2.0: mean={count_t20.mean():.1f}')\n",
    "ax.hist(count_t30, bins=20, density=True, alpha=0.7, color='steelblue',\n",
    "        label=f'|t|>3.0: mean={count_t30.mean():.1f}')\n",
    "ax.axvline(expected_t20, color='tomato', ls='--', lw=2,\n",
    "           label=f'Expected t>2: {expected_t20:.1f}')\n",
    "ax.axvline(expected_t30, color='steelblue', ls='--', lw=2,\n",
    "           label=f'Expected t>3: {expected_t30:.1f}')\n",
    "ax.set_xlabel('Number of factors with significant t-statistic')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Multiple Testing Simulation\\n({n_factors_tested} null factors, T={T_mt} months)')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Panel 2: illustrate the Bonferroni and BHY corrections\n",
    "alpha_fdr = 0.05\n",
    "n_tests_range = np.arange(1, 500)\n",
    "t_bonferroni = norm_stats.ppf(1 - alpha_fdr / (2 * n_tests_range))   # Bonferroni\n",
    "t_nominal    = np.full_like(n_tests_range, norm_stats.ppf(1 - alpha_fdr/2), dtype=float)\n",
    "# Harvey-Liu-Zhu (2016) approximate threshold\n",
    "t_hlz = 0.7071 * np.sqrt(norm_stats.ppf(1 - alpha_fdr / (2 * n_tests_range))**2 + np.log(n_tests_range))\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(n_tests_range, t_nominal, 'grey', lw=2, ls='--',\n",
    "         label='Nominal (t=1.96 for α=5%)')\n",
    "ax2.plot(n_tests_range, t_bonferroni, 'tomato', lw=2,\n",
    "         label='Bonferroni correction')\n",
    "ax2.plot(n_tests_range, np.clip(t_hlz, 2, 5), 'steelblue', lw=2,\n",
    "         label='Harvey-Liu-Zhu (2016)')\n",
    "ax2.axvline(316, color='black', ls=':', lw=1.5, label='316 factors tested (HLZ)')\n",
    "ax2.axhline(3.0, color='#27ae60', ls='-.', lw=1.5, label='t=3.0 threshold')\n",
    "ax2.set_xlabel('Number of factors tested')\n",
    "ax2.set_ylabel('Required t-statistic for significance')\n",
    "ax2.set_title('Adjusted Significance Thresholds\\nfor Multiple Hypothesis Testing')\n",
    "ax2.set_ylim(1.5, 5)\n",
    "ax2.set_xlim(1, 400)\n",
    "ax2.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Under H0 (no true factor), testing {n_factors_tested} factors with |t|>2.0:\")\n",
    "print(f\"  Expected false positives: {expected_t20:.1f}\")\n",
    "print(f\"  Simulated mean:          {count_t20.mean():.1f}\")\n",
    "print(f\"\\nWith the stricter |t|>3.0 threshold:\")\n",
    "print(f\"  Expected false positives: {expected_t30:.2f}\")\n",
    "print(f\"  Simulated mean:          {count_t30.mean():.2f}\")\n",
    "print(f\"\\nHarvey-Liu-Zhu (2016): with 316 factors, the threshold should be approximately 3.0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Factor Spanning and Model Comparison\n",
    "\n",
    "### 9.1 The Spanning Test\n",
    "\n",
    "A proposed new factor $F_{new}$ is **spanned** by existing factors $\\mathbf{F}$ if its alpha in a time-series regression on $\\mathbf{F}$ is zero:\n",
    "$$\n",
    "F_{new,t} = \\alpha + \\boldsymbol{\\beta}' \\mathbf{F}_t + \\varepsilon_t, \\quad \\alpha = 0.\n",
    "$$\n",
    "If $\\alpha = 0$, the new factor does not improve the maximum Sharpe ratio achievable from the existing factors. It is redundant.\n",
    "\n",
    "**Example**: Fama and French (2015) showed that HML is largely spanned by the other four factors (especially CMA and RMW) for many test portfolios. This raised the question of whether the five-factor model could be reduced to four factors without loss of pricing power.\n",
    "\n",
    "### 9.2 Cross-Sectional R-Squared\n",
    "\n",
    "A simple descriptive measure of model fit is the cross-sectional $R^2$ from the Fama-MacBeth second-pass regression. It measures the fraction of the cross-sectional variation in mean returns explained by the factor betas:\n",
    "$$\n",
    "R^2_{CS} = 1 - \\frac{\\sum_i (\\bar{R}^i - \\hat{\\lambda}_0 - \\hat{\\boldsymbol{\\lambda}}' \\hat{\\boldsymbol{\\beta}}_i)^2}{\\sum_i (\\bar{R}^i - \\bar{R})^2}.\n",
    "$$\n",
    "The CAPM achieves $R^2_{CS} \\approx 0.01$ on 25 size-value portfolios (essentially zero — betas have no cross-sectional explanatory power). FF3 achieves $R^2_{CS} \\approx 0.80$. FF5 achieves $R^2_{CS} \\approx 0.90$.\n",
    "\n",
    "However, the cross-sectional $R^2$ can be artificially inflated by including irrelevant factors. It should be interpreted alongside the GRS test and the HJ distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanning test: is a new factor redundant given existing ones?\n",
    "# Compare model pricing performance: CAPM vs FF3 vs FF5 (cross-sectional R2)\n",
    "\n",
    "# Fama-MacBeth on the 25 simulated portfolios\n",
    "def fama_macbeth(R_test, F_factors, T_first_pass):\n",
    "    \"\"\"\n",
    "    Two-pass Fama-MacBeth regression.\n",
    "    Returns: lambda estimates, CS R2, mean |alpha|\n",
    "    \"\"\"\n",
    "    T, N = R_test.shape\n",
    "    K = F_factors.shape[1]\n",
    "\n",
    "    # Pass 1: estimate betas in first T_first_pass periods\n",
    "    X_ts = np.column_stack([np.ones(T_first_pass), F_factors[:T_first_pass]])\n",
    "    betas_hat = np.empty((N, K))\n",
    "    for i in range(N):\n",
    "        coef = np.linalg.lstsq(X_ts, R_test[:T_first_pass, i], rcond=None)[0]\n",
    "        betas_hat[i] = coef[1:]\n",
    "\n",
    "    # Pass 2: cross-sectional regression each period in second half\n",
    "    X_cs = np.column_stack([np.ones(N), betas_hat])\n",
    "    T2 = T - T_first_pass\n",
    "    lambdas_t = np.empty((T2, K + 1))\n",
    "    for t in range(T2):\n",
    "        lam = np.linalg.lstsq(X_cs, R_test[T_first_pass + t], rcond=None)[0]\n",
    "        lambdas_t[t] = lam\n",
    "\n",
    "    lam_mean = lambdas_t.mean(axis=0)\n",
    "    lam_se   = lambdas_t.std(axis=0) / np.sqrt(T2)\n",
    "    t_stats  = lam_mean / lam_se\n",
    "\n",
    "    # Cross-sectional R2\n",
    "    R_bar = R_test[T_first_pass:].mean(axis=0)  # mean returns in second half\n",
    "    R_hat = X_cs @ lam_mean  # fitted\n",
    "    ss_res = np.sum((R_bar - R_hat)**2)\n",
    "    ss_tot = np.sum((R_bar - R_bar.mean())**2)\n",
    "    cs_r2  = 1 - ss_res / ss_tot\n",
    "    mae    = np.abs(R_bar - R_hat).mean() * 1200\n",
    "\n",
    "    return {\n",
    "        'lambda_mean': lam_mean * 1200,   # annualised\n",
    "        'lambda_se':   lam_se * 1200,\n",
    "        't_stats':     t_stats,\n",
    "        'cs_r2':       cs_r2,\n",
    "        'mae':         mae,\n",
    "        'R_bar':       R_bar * 1200,\n",
    "        'R_hat':       R_hat * 1200,\n",
    "        'betas':       betas_hat,\n",
    "    }\n",
    "\n",
    "T_first = T_grs // 2\n",
    "fm_capm = fama_macbeth(R_grs, F_grs[:, [0]],       T_first)\n",
    "fm_ff3  = fama_macbeth(R_grs, F_grs[:, [0,1,2]],  T_first)\n",
    "fm_ff5  = fama_macbeth(R_grs, F_grs,              T_first)\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"Fama-MacBeth Cross-Sectional Results (annualised %)\")\n",
    "print(\"=\" * 65)\n",
    "for name, fm, factor_subset in [('CAPM', fm_capm, ['MKT']),\n",
    "                                   ('FF3',  fm_ff3,  ['MKT','SMB','HML']),\n",
    "                                   ('FF5',  fm_ff5,  factor_names)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Cross-sectional R²: {fm['cs_r2']:.3f}\")\n",
    "    print(f\"  Mean abs error:     {fm['mae']:.3f}% p.a.\")\n",
    "    labs = ['Intercept'] + factor_subset\n",
    "    for lab, lam, se, t in zip(labs, fm['lambda_mean'], fm['lambda_se'], fm['t_stats']):\n",
    "        sig = '***' if abs(t) > 3 else ('**' if abs(t) > 2 else '')\n",
    "        print(f\"  λ_{lab:6s} = {lam:6.2f}%  (t = {t:5.2f}) {sig}\")\n",
    "\n",
    "# Visualise cross-sectional fit\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (name, fm) in zip(axes, [('CAPM', fm_capm), ('FF3', fm_ff3), ('FF5', fm_ff5)]):\n",
    "    mn, mx = min(fm['R_bar'].min(), fm['R_hat'].min()) - 0.5, \\\n",
    "             max(fm['R_bar'].max(), fm['R_hat'].max()) + 0.5\n",
    "    ax.scatter(fm['R_hat'], fm['R_bar'],\n",
    "               c=size_idx, cmap='Reds', s=80, edgecolors='black', lw=0.4, zorder=5)\n",
    "    ax.plot([mn, mx], [mn, mx], 'k--', lw=1.5)\n",
    "    ax.set_xlabel('Predicted mean return (% p.a.)')\n",
    "    ax.set_ylabel('Realised mean return (% p.a.)')\n",
    "    ax.set_title(f'{name}\\n$R^2_{{CS}}$={fm[\"cs_r2\"]:.2f}, MAE={fm[\"mae\"]:.2f}% p.a.')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Cross-Sectional Fit: 25 Size-Value Portfolios\\n'\n",
    "             '(color shade = size quintile, dark=small)',\n",
    "             fontsize=12, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. The Momentum Factor: An Anomaly That Resists Explanation\n",
    "\n",
    "### 10.1 The Momentum Phenomenon\n",
    "\n",
    "Jegadeesh and Titman (1993) documented that stocks with high returns over the past 3–12 months continue to outperform stocks with low past returns over the next 3–12 months. This **momentum effect** is one of the strongest and most pervasive anomalies in finance.\n",
    "\n",
    "The standard construction of the **UMD** (Up Minus Down, or WML — Winners Minus Losers) factor:\n",
    "1. Each month, rank stocks by their 12-month cumulative return, excluding the most recent month (the \"12-1\" return to avoid short-term reversal).\n",
    "2. Go long the top 30% (winners) and short the bottom 30% (losers).\n",
    "3. The return on this zero-cost portfolio is UMD.\n",
    "\n",
    "Historical properties of UMD (US, 1963–2023, approximate):\n",
    "- Mean return: ~7.5% per year.\n",
    "- Standard deviation: ~16% per year.\n",
    "- Sharpe ratio: ~0.47.\n",
    "- **Correlation with MKT, SMB, HML: near zero** — momentum is not spanned by the Fama-French factors.\n",
    "\n",
    "### 10.2 Why Momentum Is Puzzling\n",
    "\n",
    "The momentum premium is puzzling for two distinct reasons:\n",
    "\n",
    "**Reason 1 — Risk-based explanation is difficult**: Any risk-based explanation requires that winner portfolios have higher SDF exposure in bad times than loser portfolios. But momentum portfolios are highly dynamic: last year's winners become this year's losers over a 2–5 year horizon (long-run reversal, DeBondt-Thaler 1985). It is hard to construct a stable risk story for a factor that reverses sign over medium horizons.\n",
    "\n",
    "**Reason 2 — Momentum crashes**: The momentum portfolio exhibits extreme **negative skewness** and periodic **crashes**. In market rebounds following steep declines, past losers (which have fallen furthest and have the highest beta to a rebound) outperform past winners dramatically. The most severe momentum crash occurred in 2009: the UMD factor returned approximately -83% in three months (March-May 2009). These crashes are inconsistent with a risk-based explanation under standard preferences — if momentum is a risk factor, it should pay off in bad times, not crash precisely when the market recovers.\n",
    "\n",
    "**Behavioural explanations**: Daniel, Hirshleifer, and Subrahmanyam (1998) proposed that momentum arises from investor overconfidence — investors overreact to their own private signals, driving up prices of recent winners. The overreaction eventually reverses (long-run mean reversion). Barberis, Shleifer, and Vishny (1998) proposed underreaction to public earnings news as the mechanism.\n",
    "\n",
    "Momentum remains the most significant unexplained anomaly in the cross-section of expected returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate momentum-like returns and illustrate the crash risk\n",
    "# Use a regime-switching model: normal regime + crash regime\n",
    "\n",
    "T_mom = 12 * 50   # 50 years monthly\n",
    "\n",
    "# Parameters calibrated to approximate UMD stylised facts\n",
    "mu_mom_normal  = 0.008    # monthly mean in normal regime (~9.6% p.a.)\n",
    "mu_mom_crash   = -0.15    # monthly mean in crash regime (-15% per month)\n",
    "sigma_mom_normal = 0.04   # monthly vol in normal regime\n",
    "sigma_mom_crash  = 0.12   # monthly vol in crash regime\n",
    "p_crash = 0.005           # probability of being in crash regime each month\n",
    "p_recover = 0.30          # probability of leaving crash regime each month\n",
    "\n",
    "# Simulate regime\n",
    "regime = np.zeros(T_mom, dtype=int)  # 0 = normal, 1 = crash\n",
    "for t in range(1, T_mom):\n",
    "    if regime[t-1] == 0:\n",
    "        regime[t] = int(rng.random() < p_crash)\n",
    "    else:\n",
    "        regime[t] = int(rng.random() > p_recover)\n",
    "\n",
    "# Draw returns\n",
    "eps_mom = rng.standard_normal(T_mom)\n",
    "ret_mom = np.where(regime == 0,\n",
    "                   mu_mom_normal + sigma_mom_normal * eps_mom,\n",
    "                   mu_mom_crash  + sigma_mom_crash  * eps_mom)\n",
    "\n",
    "# Cumulative return\n",
    "cum_mom = np.cumsum(ret_mom)\n",
    "years_mom = np.arange(T_mom) / 12\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(years_mom, cum_mom * 100, color='#27ae60', lw=1.2)\n",
    "# Shade crash periods\n",
    "crash_periods = regime == 1\n",
    "for t in range(T_mom):\n",
    "    if crash_periods[t]:\n",
    "        ax.axvspan(years_mom[t], years_mom[t] + 1/12, alpha=0.25, color='red')\n",
    "ax.set_xlabel('Year'); ax.set_ylabel('Cumulative return (pp)')\n",
    "ax.set_title('Simulated Momentum Factor (UMD)\\nRed bands = crash regime')\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(ret_mom * 100, bins=80, density=True, color='#27ae60', alpha=0.75, edgecolor='none')\n",
    "# Normal density for comparison\n",
    "x_grid = np.linspace(-45, 25, 300)\n",
    "normal_pdf = norm.pdf(x_grid, ret_mom.mean() * 100, ret_mom.std() * 100)\n",
    "ax2.plot(x_grid, normal_pdf, 'k--', lw=2, label='Normal approximation')\n",
    "ax2.set_xlabel('Monthly return (%)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Return Distribution\\nHeavy left tail (crash risk)')\n",
    "ax2.set_xlim(-45, 25)\n",
    "\n",
    "# Compute moments\n",
    "from scipy.stats import skew, kurtosis\n",
    "mu_ann  = ret_mom.mean() * 1200\n",
    "sig_ann = ret_mom.std() * np.sqrt(12) * 100\n",
    "sk      = skew(ret_mom)\n",
    "ex_kurt = kurtosis(ret_mom)  # excess kurtosis\n",
    "ax2.text(0.05, 0.90, f'Mean: {mu_ann:.1f}% p.a.\\nVol: {sig_ann:.1f}% p.a.\\n'\n",
    "         f'Skew: {sk:.2f}\\nEx.Kurt: {ex_kurt:.2f}',\n",
    "         transform=ax2.transAxes, fontsize=9,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "ax2.legend()\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "# Rolling Sharpe ratio (24-month window)\n",
    "window = 24\n",
    "rolling_sr = np.array([\n",
    "    (ret_mom[t:t+window].mean() / ret_mom[t:t+window].std()) * np.sqrt(12)\n",
    "    for t in range(T_mom - window)\n",
    "])\n",
    "ax3.plot(years_mom[window:], rolling_sr, color='#8e44ad', lw=1)\n",
    "ax3.axhline(0, color='black', lw=0.8, ls='--')\n",
    "ax3.axhline(mu_ann / sig_ann, color='grey', lw=1, ls=':',\n",
    "            label=f'Full-sample SR = {mu_ann/sig_ann:.2f}')\n",
    "ax3.set_xlabel('Year'); ax3.set_ylabel('Rolling Sharpe ratio (annualised)')\n",
    "ax3.set_title('Rolling 24-Month Sharpe Ratio\\nExtreme time variation — not a stable risk factor')\n",
    "ax3.legend()\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "# Drawdown profile\n",
    "cum_level = np.exp(cum_mom)\n",
    "running_max = np.maximum.accumulate(cum_level)\n",
    "drawdown = (cum_level - running_max) / running_max * 100\n",
    "ax4.fill_between(years_mom, drawdown, 0, color='tomato', alpha=0.6)\n",
    "ax4.set_xlabel('Year'); ax4.set_ylabel('Drawdown (%)')\n",
    "ax4.set_title('Drawdown Profile\\nMomentum crashes can exceed 50%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Momentum Factor: High Mean Return but Crash Risk',\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Momentum annual moments:\")\n",
    "print(f\"  Mean:          {mu_ann:.1f}%\")\n",
    "print(f\"  Vol:           {sig_ann:.1f}%\")\n",
    "print(f\"  Sharpe:        {mu_ann/sig_ann:.2f}\")\n",
    "print(f\"  Skewness:      {sk:.2f}  (negative = crash risk)\")\n",
    "print(f\"  Excess kurtosis: {ex_kurt:.2f}  (fat tails)\")\n",
    "print(f\"\\nFraction of months in crash regime: {regime.mean()*100:.1f}%\")\n",
    "print(f\"Worst single-month return:          {ret_mom.min()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary and Looking Ahead\n",
    "\n",
    "This week covered the quantitative frontier of equilibrium models and the empirical structure of expected returns.\n",
    "\n",
    "**Part A — Long-Run Risks**: The Bansal-Yaron (2004) model introduces a small but persistent component $x_t$ in consumption growth and stochastic volatility $\\sigma^2_t$. Combined with Epstein-Zin preferences ($\\psi > 1$, $\\gamma \\approx 10$), this generates:\n",
    "- A large equity premium through the long-run risk channel: agents with $\\psi > 1$ strongly dislike news about *future* consumption, and this sensitivity amplifies the SDF.\n",
    "- A reasonable riskfree rate because $\\psi > 1$ decouples the riskfree rate from risk aversion.\n",
    "- Time-varying risk premia via stochastic volatility, with countercyclical premia.\n",
    "- The critical condition: $A_1 > 0$ (good news raises the price-consumption ratio) requires $\\psi > 1$.\n",
    "\n",
    "**Part B — Disaster Risk**: Barro (2006) shows that a 1.7% annual probability of a 22%+ consumption contraction (calibrated from 35 countries' 20th-century history) generates a 4–6% equity premium at $\\gamma \\approx 3$–4 — far more modest than the CCAPM requires. The disaster premium dominates the normal-times premium. The peso problem — disasters are rare in the US postwar sample — means the model cannot be directly rejected by US data alone.\n",
    "\n",
    "**Part C — Factor Models**: The APT (Ross, 1976) provides a no-arbitrage foundation: in a factor return structure with diversified idiosyncratic risk, expected returns must be linear in factor betas. The SDF is correspondingly linear in the factors. The Fama-French three-factor (1993) and five-factor (2015) models are the standard empirical implementations, dramatically improving on the CAPM's cross-sectional fit. The GRS test provides a formal joint test of all pricing errors. The factor zoo problem — over 300 proposed factors in the literature — requires multiple-testing corrections (Harvey-Liu-Zhu threshold of $t \\approx 3.0$) and out-of-sample validation. Momentum (Jegadeesh-Titman, 1993) remains the most prominent anomaly: large premium, crash risk, and no convincing risk-based explanation.\n",
    "\n",
    "**In Week 4**, we examine the empirical failures of factor models in more depth: the anomalies that challenge the Fama-French models (low volatility, quality, betting-against-beta), the term structure of risk premia, and the time-series predictability of equity returns.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Set — Week 3\n",
    "\n",
    "**Problem 1 — Long-Run Risks and the IES**: In the Bansal-Yaron model, the coefficient $A_1$ (loading of the log price-consumption ratio on the long-run component $x_t$) is:\n",
    "$$\n",
    "A_1 = \\frac{1 - 1/\\psi}{1 - \\kappa_1 \\rho}.\n",
    "$$\n",
    "(a) Show that $A_1 > 0$ if and only if $\\psi > 1$. (b) Explain why the sign of $A_1$ determines whether news about future consumption growth raises or lowers equity prices. (c) With $\\psi = 0.5$, $\\rho = 0.979$, and $\\kappa_1 = 0.997$, compute $A_1$ and interpret the sign. (d) How does the persistence $\\rho$ amplify the magnitude of $A_1$?\n",
    "\n",
    "**Problem 2 — Disaster Risk Calibration**: Using the disaster moments from Section 3 (p = 0.017, log-uniform $b \\in [0.15, 0.64]$, $\\phi = 3$): (a) Compute the disaster risk premium analytically for $\\gamma \\in \\{2, 4, 6\\}$. (b) What leverage $\\phi$ is needed to generate a 6% equity premium at $\\gamma = 3$? (c) How sensitive is the equity premium to the disaster probability $p$? Compute $\\partial (\\text{EP}) / \\partial p$ at $p = 0.017$, $\\gamma = 3$.\n",
    "\n",
    "**Problem 3 — The APT spanning test**: You observe four factors: $F_1$ (market), $F_2$ (size), $F_3$ (value), $F_4$ (momentum). Run the time-series regression $F_4 = \\alpha + \\beta_1 F_1 + \\beta_2 F_2 + \\beta_3 F_3 + \\varepsilon$ and test $H_0: \\alpha = 0$. (a) If $\\alpha = 0$, what does this imply about the SDF representation of the four-factor model? (b) Generate simulated factor returns with the correlation structure from Section 6 (but adding a momentum factor with near-zero correlation to FF5 factors). What is the $t$-statistic on $\\alpha$ in the spanning regression?\n",
    "\n",
    "**Problem 4 — GRS Test Power**: Using the simulation from Section 7, (a) compute the power of the GRS test against the alternative that CAPM alphas are as large as estimated from the simulation. (b) How many years of monthly data are needed to achieve 80% power against the same alternative? (c) Discuss why the GRS test may reject the true model when the number of test portfolios $N$ is large relative to $T$.\n",
    "\n",
    "**Problem 5 — Multiple Testing Correction**: Suppose a researcher tests 50 factors and finds 8 with $|t| > 2.0$. (a) Compute the expected number of false positives under the null using the Bonferroni correction. (b) What is the Bonferroni-adjusted significance threshold for each individual test at a 5% family-wise error rate? (c) Under the Harvey-Liu-Zhu framework, what $t$-statistic threshold should the researcher apply? (d) How many of the 8 \"significant\" factors likely survive this threshold?\n",
    "\n",
    "**Problem 6 — Momentum and Crash Risk**: The momentum factor UMD has approximately the following monthly return distribution: normal-times regime (probability 0.99) with mean 0.8% and volatility 4%, and crash regime (probability 0.01) with mean -15% and volatility 12%. (a) Compute the unconditional mean, variance, skewness, and excess kurtosis of monthly UMD returns analytically. (b) Compute the unconditional Sharpe ratio (annualised). (c) A risk-averse investor with $\\gamma = 5$ and CRRA utility considers allocating to UMD. Compute the certainty-equivalent improvement from holding the optimal allocation in UMD alongside a riskfree asset, relative to holding the riskfree asset alone. (d) Does the skewness of UMD make it more or less attractive than a normally distributed factor with the same mean and variance? Explain.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Bansal, R. and Yaron, A. (2004). Risks for the long run: A potential resolution of asset pricing puzzles. *Journal of Finance*, 59(4), 1481–1509.\n",
    "- Rietz, T. A. (1988). The equity risk premium: A solution. *Journal of Monetary Economics*, 22(1), 117–131.\n",
    "- Barro, R. J. (2006). Rare disasters and asset markets in the twentieth century. *Quarterly Journal of Economics*, 121(3), 823–866.\n",
    "- Nakamura, E., Steinsson, J., Barro, R. and Ursua, J. (2013). Crises and recoveries in an empirical model of consumption disasters. *American Economic Journal: Macroeconomics*, 5(3), 35–74.\n",
    "- Ross, S. A. (1976). The arbitrage theory of capital asset pricing. *Journal of Economic Theory*, 13(3), 341–360.\n",
    "- Fama, E. F. and French, K. R. (1993). Common risk factors in the returns on stocks and bonds. *Journal of Financial Economics*, 33(1), 3–56.\n",
    "- Fama, E. F. and French, K. R. (2015). A five-factor asset pricing model. *Journal of Financial Economics*, 116(1), 1–22.\n",
    "- Jegadeesh, N. and Titman, S. (1993). Returns to buying winners and selling losers: Implications for stock market efficiency. *Journal of Finance*, 48(1), 65–91.\n",
    "- Harvey, C. R., Liu, Y. and Zhu, H. (2016). ... and the cross-section of expected returns. *Review of Financial Studies*, 29(1), 5–68.\n",
    "- Gibbons, M. R., Ross, S. A. and Shanken, J. (1989). A test of the efficiency of a given portfolio. *Econometrica*, 57(5), 1121–1152.\n",
    "- McLean, R. D. and Pontiff, J. (2016). Does academic research destroy stock return predictability? *Journal of Finance*, 71(1), 5–32.\n",
    "- Cochrane, J. H. (2011). Presidential address: Discount rates. *Journal of Finance*, 66(4), 1047–1108.\n",
    "- Gabaix, X. (2012). Variable rare disasters: An exactly solved framework for ten puzzles in macro-finance. *Quarterly Journal of Economics*, 127(2), 645–700.\n",
    "- Bollerslev, T., Tauchen, G. and Zhou, H. (2009). Expected stock returns and variance risk premia. *Review of Financial Studies*, 22(11), 4463–4492."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
